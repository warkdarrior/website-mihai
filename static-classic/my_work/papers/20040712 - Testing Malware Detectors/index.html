<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <title>Testing Malware Detectors</title>

    <link type="text/css" rel="stylesheet" href="http://www.cs.wisc.edu/~mihai/default_style.css">
    <style type="text/css">
<!--#if expr="${HTTP_USER_AGENT} = /MSIE/" -->
      BODY { }

      BODY A:link { color: #007700; padding-left: 2px; padding-right: 2px; }
      BODY A:visited { color: #007700; }
      BODY A:hover { background-color: #005500; color: #ffffff; }

      .Paper { text-align: justify; padding-left: 72px; padding-right: 72px; width: 750px; border: 1px solid black; background-color: #fffff0; font-family: bitstream vera serif, Georgia, serif; font-size: 10pt; }

      H1.Title { text-align: center; }

      .TitleNote { font-size: smaller; border: 1px solid #a9a99a; padding: 0.35em; margin: 1em 0px 0px 300px; background-color: #fefeef; color: #a9a99a; font-family: bitstream vera sans, Arial, sans-serif; font-size: 10pt; position: relative; right: -62px; }
      .ACMNote { font-size: smaller; border: 1px solid #a9a99a; padding: 0.35em; margin: 1em 0px 0px 300px; background-color: #fefeef; color: #a9a99a; font-family: bitstream vera sans, Arial, sans-serif; font-size: 10pt; position: relative; right: -62px; }

      .Authors { text-align: center; }
      .Author { font-size: larger; }
      .AuthorAddress { }
      .AuthorDepartment { }
      .AuthorSchool { }
      .AuthorSchoolStreet { }
      .AuthorSchoolCity { }
      .AuthorEmails { font-family: bitstream vera sans mono, Lucida Console, monospace; font-size: 10pt; }

      .Abstract { margin: 1em auto 1em auto; }
      .Abstract .Title { text-transform: uppercase; font-weight: bold; font-size: larger; position: relative; left: -2em; }

      .ACMCategories { margin: 1em 0 1em 0; }
      .ACMCategories .Title { font-weight: bold; font-size: larger; }
      .ACMCategories .Id { }
      .ACMCategories .ParentCategory { font-weight: bold; }

      .ACMTerms { margin: 1em 0 1em 0; }
      .ACMTerms .Title { font-weight: bold; font-size: larger; }

      .ACMKeywords { margin: 1em 0 1em 0; }
      .ACMKeywords .Title { font-weight: bold; font-size: larger; }

      .Section { margin: 2em auto 1em auto; }
      .Section .Title { text-transform: uppercase; font-weight: bold; font-size: larger; position: relative; left: -2em; }

      .SubSection { margin: 2em 0 1em 0em; }
      .SubSection .Title { font-weight: bold; font-size: larger; position: relative; left: -2em; }

      .SubSubSection { margin: 2em 0 1em 0; }
      .SubSubSection .Title { font-style: italic; font-size: larger; position: relative; left: -2em; }

      .Notes { margin: 2em 0 1em 0; }
      .Notes .Title { text-transform: uppercase; font-weight: bold; font-size: larger; position: relative; left: -2em; }
      .Notes OL { padding-left: 0em; }
      .Notes OL LI { margin-top: 1em; margin-bottom: 1em; }

      .Bibliography { margin: 2em 0 1em 0; }
      .Bibliography .Title { text-transform: uppercase; font-weight: bold; font-size: larger; position: relative; left: -2em; }
      .Bibliography OL { padding-left: 0em; }
      .Bibliography OL LI.Item { margin-top: 1em; margin-bottom: 1em; }
      .Bibliography OL LI.Item + LI.Item { border-top: 1px dashed #aaaaaa; padding-top: 1em; margin-bottom: 1em; }

      .Math { background-color: #eeffee; font-family: bitstream vera sans, Tahoma, sans-serif; font-size: 10pt; }

      .Listing { }
      .Listing .Caption { margin-top: 0.5em; max-width: 750px; text-align: left; font-size: 10pt; }
      .Listing PRE { border: 1px solid black; margin: 0; padding: 0.5em; }

      .Algorithm { font-size: 10pt; }
      .Algorithm .Caption { margin-top: 0.5em; max-width: 750px; text-align: left; font-size: 10pt; }
      .Algorithm .Body { border: 1px solid black; margin: 0; padding: 0.5em; }
      .Algorithm .Declaration { margin-top: 1em; }
      .Algorithm .Declaration .AlgoName { font-variant: small-caps; text-decoration: underline; }
      .Algorithm .Declaration .Argument { background-color: #eeeeee; }
      .Algorithm .AlgoScope { margin-left: 2em; }

      .Definition { font-style: italic; padding-left: 2em; }
      .Definition .Header { margin-left: -2em; }
      .Definition .Label { font-style: normal; font-variant: small-caps; }
      .Definition .Title { font-style: normal; font-style: none; padding-left: 2em; }
      .Definition .Math { font-style: normal; background-color: #eeffee; }

      .TableFloat TBODY TD { padding-left: 1em; padding-right: 1em; padding-top: .15em; padding-bottom: .15em; font-size: 10pt; }
      .TableFloat THEAD TD { padding-left: 1em; padding-right: 1em; padding-top: .15em; padding-bottom: .15em; font-size: 10pt; font-style: italic; background-color: #eeeee0; }
      .TableFloat .Caption { margin-top: 0.5em; max-width: 750px; text-align: left; }

      .FigureFloat .Caption { margin-top: 0.5em; max-width: 750px; text-align: left; font-size: 10pt; }
<!--#else -->
      BODY { }

      BODY A:link { color: #007700; padding-left: 2px; padding-right: 2px; }
      BODY A:visited { color: #007700; }
      BODY A:hover { background-color: #005500; color: #ffffff; }

      .Paper { text-align: justify; margin: 0em; padding: 0.25in 1in 0.25in 1in; width: 6.5in; border: 1px solid black; background-color: #fffff0; font-family: bitstream vera serif, Georgia, serif; font-size: 10pt; }

      H1.Title { text-align: center; }

      .TitleNote { width: 50%; font-size: smaller; border: 1px solid #a9a99a; padding: 0.35em; margin: 0.25em 0px 0.25em auto; background-color: #fefeef; color: #a9a99a; font-family: bitstream vera sans, Arial, sans-serif; position: relative; left: 0.75in; font-size: 10pt; }
      .ACMNote { width: 50%; font-size: smaller; border: 1px solid #a9a99a; padding: 0.35em; margin: 1em 0px 1.25em auto; background-color: #fefeef; color: #a9a99a; font-family: bitstream vera sans, Arial, sans-serif; position: relative; left: 0.75in; font-size: 10pt; }

      .Authors { text-align: center; }
      .Author { font-size: larger; }
      .AuthorAddress { }
      .AuthorDepartment { }
      .AuthorSchool { }
      .AuthorSchoolStreet { }
      .AuthorSchoolCity { }
      .AuthorEmails { font-family: bitstream vera sans mono, Lucida Console, monospace; font-size: 10pt; }

      .Abstract { width: 75%; margin: 1em auto 1em auto; }
      .Abstract .Title { text-transform: uppercase; font-weight: bold; font-size: larger; }

      .ACMCategories { margin: 1em 0 1em 0; }
      .ACMCategories .Title { font-weight: bold; font-size: larger; }
      .ACMCategories .Id { }
      .ACMCategories .ParentCategory { font-weight: bold; }

      .ACMTerms { margin: 1em 0 1em 0; }
      .ACMTerms .Title { font-weight: bold; font-size: larger; }

      .ACMKeywords { margin: 1em 0 1em 0; }
      .ACMKeywords .Title { font-weight: bold; font-size: larger; }

      .Section { margin: 2em 0 1em 0; }
      .Section > .Title { text-transform: uppercase; font-weight: bold; font-size: larger; position: relative; left: -2em; }

      .SubSection { margin: 2em 0 1em 0em; }
      .SubSection > .Title { font-weight: bold; font-size: larger; position: relative; left: -2em; }

      .SubSubSection { margin: 2em 0 1em 0; }
      .SubSubSection > .Title { font-style: italic; font-size: larger; position: relative; left: -2em; }

      .Notes { margin: 2em 0 1em 0; }
      .Notes > .Title { text-transform: uppercase; font-weight: bold; font-size: larger; position: relative; left: -2em; }
      .Notes > OL { padding-left: 0em; }
      .Notes > OL > LI { margin-top: 1em; margin-bottom: 1em; }

      .Bibliography { margin: 2em 0 1em 0; }
      .Bibliography > .Title { text-transform: uppercase; font-weight: bold; font-size: larger; position: relative; left: -2em; }
      .Bibliography > OL { padding-left: 0em; }
      .Bibliography > OL > LI.Item { margin-top: 1em; margin-bottom: 1em; }
      .Bibliography > OL > LI.Item + LI.Item { border-top: 1px dashed #aaaaaa; padding-top: 1em; margin-bottom: 1em; }

      .Math { background-color: #eeffee; font-family: bitstream vera sans, Tahoma, sans-serif; font-size: 10pt; }

      .Listing { }
      .Listing .Caption { margin-top: 0.5em; max-width: 6.5in; text-align: left; font-size: 10pt; }
      .Listing PRE { border: 1px solid black; margin: 0; padding: 0.5em; }

      .Algorithm { font-size: 10pt; }
      .Algorithm .Caption { margin-top: 0.5em; max-width: 6.5in; text-align: left; font-size: 10pt; }
      .Algorithm .Body { border: 1px solid black; margin: 0; padding: 0.5em; }
      .Algorithm .Declaration { margin-top: 1em; }
      .Algorithm .Declaration .AlgoName { font-variant: small-caps; text-decoration: underline; }
      .Algorithm .Declaration .Argument { background-color: #eeeeee; }
      .Algorithm .AlgoScope { margin-left: 2em; }

      .Definition { font-style: italic; padding-left: 2em; }
      .Definition .Header { margin-left: -2em; }
      .Definition .Label { font-style: normal; font-variant: small-caps; }
      .Definition .Title { font-style: normal; font-style: none; padding-left: 2em; }
      .Definition .Math { font-style: normal; background-color: #eeffee; }

      .TableFloat TBODY TD { padding-left: 1em; padding-right: 1em; padding-top: .15em; padding-bottom: .15em; font-size: 10pt; }
      .TableFloat THEAD TD { padding-left: 1em; padding-right: 1em; padding-top: .15em; padding-bottom: .15em; font-size: 10pt; font-style: italic; background-color: #eeeee0; }
      .TableFloat .Caption { margin-top: 0.5em; max-width: 6.5in; text-align: left; }

      .FigureFloat .Caption { margin-top: 0.5em; max-width: 6.5in; text-align: left; font-size: 10pt; }
<!--#endif -->
    </style>
  </head>

  <body>

    <div class="PageHeader">
      <div style="border-bottom: 1px solid #bbaa66;">
	<div style="border-bottom: 1px solid #ddcc88;">
	  <div class="spacer"></div>
	  <div class="row">
	    <span class="left">Mihai Christodorescu: main&nbsp;page</span><span class="right"><a class="Onsite" style="margin-right: 0.5em;" href="../../../calendar.html">
                Calendar
              </a><a class="Onsite" href="../../../contact.html">
                Contact info
              </a></span>
	  </div>
	  <div class="spacer"></div>
	</div>
      </div>
    </div>

    <br>

    <center>
      <div style="text-align: right; width: 8.5in; margin-bottom: 1em;">
        For printing, I recommend the <a class="Onsite"
          href="../../../pdfs/20040712 - Testing Malware Detectors.pdf">PDF version</a>.
      </div>
    </center>

    <center>
      <div class="Paper">

        <div class="TitleNote">
          This work was supported in part by the <a class="Offsite"
          style="color: #a9c99a;"
          href="http://www.onr.navy.mil">Office of Naval Research</a>
          under contracts N00014-01-1-0796 and N00014-01-1-0708. The
          U.S. Government is authorized to reproduce and distribute
          reprints for Governmental purposes, notwithstanding any
          copyright notices affixed thereon. The views and conclusions
          contained herein are those of the authors, and should not be
          interpreted as necessarily representing the official
          policies or endorsements, either expressed or implied, of
          the above government agencies or the U.S. Government.
        </div>

        <div class="ACMNote">
          <center><i><a class="Offsite" style="color: #a9c99a;"
          href="http://www.acm.org">ACM</a> says:</i></center> &copy;
          ACM, 2004. This is the author's version of the work. It is
          posted here by permission of ACM for your personal use. Not
          for redistribution. The definitive version was published in
          Proceedings of the International Symposium on Software
          Testing and Analysis (ISSTA'04), July 12-14, 2004.  <a
          class="Offsite" style="color: #a9c99a;"
          href="http://doi.acm.org/10.1145/1007512.1007518">http://doi.acm.org/10.1145/1007512.1007518</a>.
        </div>

        <h1 class="Title">Testing Malware Detectors</h1>

        <div class="Authors">
          <div class="AuthorNames">
            <span class="Author">Mihai Christodorescu</span>
            and
            <span class="Author">Somesh Jha</span>
          </div>
          <div class="AuthorAddress">
            <div class="AuthorDepartment">Computer Sciences Department</div>
            <div class="AuthorSchool">University of Wisconsin</div>
            <div class="AuthorSchoolStreet">1210 W Dayton St</div>
            <div class="AuthorSchoolCity">Madison, WI 53706, USA</div>
          </div>
          <div class="AuthorEmails">
            {<a href="mailto:mihai@cs.wisc.edu">mihai</a>,<a href="mailto:jha@cs.wisc.edu">jha</a>}@cs.wisc.edu
          </div>
        </div>

        <div class="Abstract">
          <div class="Title">Abstract</div>
          In today's interconnected world, malware, such as worms and
          viruses, can cause havoc. A malware detector (commonly known
          as <em>virus scanner</em>) attempts to identify malware. In
          spite of the importance of malware detectors, there is a
          dearth of testing techniques for evaluating them. We present
          a technique based on program obfuscation for generating
          tests for malware detectors. Our technique is geared towards
          evaluating the resilience of malware detectors to various
          obfuscation transformations commonly used by hackers to
          disguise malware. We also demonstrate that a hacker can
          leverage a malware detector's weakness in handling
          obfuscation transformations and can extract the signature
          used by a detector for a specific malware. We evaluate three
          widely-used commercial virus scanners using our techniques
          and discover that the resilience of these scanners to
          various obfuscations is very poor.
        </div>

      <div class="ACMCategories">
	<div class="Title">Categories and Subject Descriptors</div>
	<span class="Id">D.2</span> [<span class="ParentCategory">Software</span>]: <span class="Category">Software Engineering</span> ;
	<span class="Id">D.2.5</span> [<span class="ParentCategory">Software Engineering</span>]: <span class="Category">Testing and Debugging</span> ;
	<span class="Id">D.4</span> [<span class="ParentCategory">Software</span>]: <span class="Category">Operating Systems</span> ;
	<span class="Id">D.4.6</span> [<span class="ParentCategory">Operating Systems</span>]: <span class="Category">Security and Protection</span>.
      </div>

      <div class="ACMTerms">
	<div class="Title">General Term</div>
	Algorithms, experimentation, measurement, security.
      </div>

      <div class="ACMKeywords">
	<div class="Title">Keywords</div>
	Anti-virus, adaptive testing, malware, obfuscation.
      </div>

      <!-- ................................................................... -->
      <!-- ................................................................... -->
      <!-- ................................................................... -->

      <div class="Section">
	<a name="section_1_introduction"></a>
	<div class="Title">1. Introduction</div>

	<p>
	  Malicious code can infiltrate hosts using a variety of
	  methods, such as attacks that exploit known software flaws,
	  hidden functionality in regular programs, and social
	  engineering. A <em>malware detector</em> identifies and
	  contains malware before it can reach a system or network. A
	  thorough evaluation of the quality of a malware detector has
	  to take into account the wide variety of threats that malwares
	  pose. A classification of malware with respect to its
	  propagation method and goal is given in [<a
	    href="#mcgrawmorrisett2000-taxonomy">26</a>].
	</p>

	<p>
	  This paper addresses the problem of testing malware
	  detectors. We focus on testing anti-virus software (such as
	  commercial virus scanners), but our techniques are general and
	  are applicable to other types of malware detectors.  In order
	  to understand the difficulties in testing malware detectors
	  one has to understand the <em>obfuscation-deobfuscation
	    game</em> that malware writers and developers of malware
	  detectors play against each other. As detection techniques
	  advance, malware writers use better hiding techniques to evade
	  detection; in response, developers of malware detectors deploy
	  better detection algorithms. For example, polymorphic and
	  metamorphic viruses (and, more recently, shellcodes [<a
	    href="#detristanulenspiegelmalcomvonunderduk2003-polyshellcode">9</a>])
	  are specifically designed to bypass detection tools. A
	  <em>polymorphic virus</em> ``morphs'' itself in order to evade
	  detection. A common technique to ``morph'' viruses is to
	  encrypt the malicious payload and decrypt it during execution.
	  To obfuscate the decryption routine, several transformations,
	  such as <tt>nop</tt>-insertion, code transposition (changing
	  the order of instructions and placing jump instructions to
	  maintain the original semantics), and register reassignment
	  (permuting the register allocation), are
	  applied. <em>Metamorphic viruses</em> attempt to evade
	  heuristic detection techniques by using more complex
	  obfuscations. When they replicate, these viruses change their
	  code in a variety of ways, such as code transposition,
	  substitution of equivalent instruction sequences, change of
	  conditional jumps, and register reassignment [<a
	    href="#marinescu2003-stepar">22</a>,<a
	    href="#szorferrie2001-metamorphic">33</a>,<a
	    href="#z0mbie-homepage">43</a>]. Furthermore, they can
	  ``weave'' the virus code into a host program, making detection
	  by traditional heuristics almost impossible since the virus
	  code is mixed with program code and the virus entry point is
	  no longer at the beginning of the program (these are
	  designated as entry point obscuring viruses [<a
	    href="#kaspersky2002-epo">18</a>]).
	</p>

	<p>
	  As hackers borrow and build on top of each other's
	  techniques, malwares share significant code
	  fragments. Frequently, a virus goes through a development
	  cycle where each successive version is only slightly
	  different from its predecessor. For example, the
	  <em>Sobig.A</em> through <em>Sobig.F</em> worm variants
	  (widespread during the summer of 2003) were developed
	  incrementally, with each successive iteration adding or
	  changing small features to the <em>Sobig</em> worm [<a
	  href="#lurhq2003-sobig.a">19</a>,<a
	  href="#lurhq2003-sobig.e">20</a>,<a
	  href="#lurhq2003-sobig.f">21</a>].
	</p>

	<p>
	  Given the obfuscation-deobfuscation game and the code reuse
	  practiced by virus writers, two questions arise:
	</p>

	<dl>
	  <dt><b>Question 1</b>:</dt>
	  <dd>How resistant is a malware detector to obfuscations or
	    variants of known malware?</dd>

	  <dt style="padding-top: 1em;"><b>Question 2</b>:</dt>
	  <dd>Using limitations of a malware detector in handling
	    obfuscations, can a hacker or a blackhat<sup><a
		href="#footnote:1">1</a></sup> determine its detection
	    algorithm?</dd>
	</dl>

	<p>
	  Question 1 is motivated by the obfuscation-deobfuscation
	  game. Question 2 is motivated by the fact that if a blackhat
	  knows the detection algorithm used by a malware detector, they
	  can better target their evasion techniques.  In other words,
	  the ``stealth'' of the detection algorithm is important.
	</p>

	<p>
	  This papers addresses the above-mentioned questions and makes
	  the following contributions:
	</p>

	<ul>
	  <li>
	    <b>Test-case generation using program obfuscation:</b>
	    Motivated by protection of intellectual property, several
	    researchers have studied program obfuscation to make reverse
	    engineering of code
	    harder [<a href="#collbergthomborsonlow1997-taxonomy">5</a>]. We use
	    transformations from the program obfuscation literature to
	    generate test cases for malware detectors. We have developed
	    an obfuscator (see <a href="#sec:obfuscation">Section 3</a>)
	    for Visual Basic programs. Given a known malware, we
	    generate several variants of it by applying various
	    obfuscation transformations. These variants are then treated
	    as test cases for malware detectors. Our obfuscation-based
	    testing technique is motivated by Question 1. Our testing
	    methodology is described in <a
	      href="#sec:methodology">Section 4</a>.
	  </li>

	  <li style="padding-top: 1em;">
	    <b>Signature extraction:</b> Motivated by question 2, we
	    have developed a signature-extraction algorithm which uses a
	    malware detector, such as a virus scanner, as a black
	    box. Using the output of a malware detector on several
	    obfuscations of a malware, we extract signature used by the
	    detection algorithm. This signature-extraction algorithm is
	    described in <a href="#sec:signature-extraction">Section
	      4.2</a>.
	  </li>

	  <li style="padding-top: 1em;">
	    <b>Experimental evaluation:</b> Using our testing
	    methodology we evaluated three commercial virus scanners:
	    Symantec's Norton AntiVirus version 7.61.934 for Windows
	    2000, Network Associates Technology Inc.'s McAfee Virus Scan
	    version 4.24.0 for Linux, and Sophos Plc's Sophos Antivirus
	    version 3.76 (engine version 2.17) for Linux/Intel. The
	    resilience of these virus scanners to various obfuscation
	    transformations was quite varied. Results of our testing
	    effort are summarized in <a href="#sec:experiments">Section
	      5</a>. Using our signature-extraction algorithm, we were
	    also able to extract signatures used by the virus scanners
	    for various malware. The extracted signatures are presented
	    in <a href="#sec:experiments">Section 5</a>. From our
	    experimental results we conclude that the state of the art
	    for malware detectors is dismal!
	  </li>
	</ul>

      </div>

      <!-- ................................................................... -->
      <!-- ................................................................... -->
      <!-- ................................................................... -->

      <div class="Section">
	<a name="section_2_related_work"></a>
	<div class="Title">2. Related Work</div>

	<p>
	  The testing technique in this paper draws upon three major
	  areas: testing of malware detectors, general software testing,
	  and program obfuscation. In the following subsections we
	  describe related work from these areas and highlight the
	  contributions presented in this paper.
	</p>

	<!-- ................................................................... -->

	<div class="SubSection">
	  <a name="section_2.1_related_malware_detector_testing"></a>
	  <div class="Title">2.1. Related work on testing malware detectors</div>

	  <p>
	    As the anti-virus software industry ramped up in the early
	    1990s, the testing and reviewing procedures trailed the
	    spread of viruses and the advances in detection
	    algorithms. As late as 1996, Sarah Gordon opened her paper
	    [<a href="#gordonford1996-avreview">13</a>] on the state of
	    affairs of anti-virus testing with the following
	    observation:
	  </p>

	  <quote>
	    ``The evaluation of anti-virus software is not adequately
	    covered by any existing criteria based on formal
	    methods. The process, therefore, has been carried out by
	    various personnel using a variety of tools and methods.''
	  </quote>

	  <p>
	    Although there is no shortage of benign programs, the major
	    difficulty in testing malware detectors is finding a suite
	    of malicious programs. Motivated by this difficulty various
	    researchers have categorized known malicious programs, <span
	      class="Math">M<sub>known</sub></span>, into malware that is
	    in the wild (<em>ITW</em>), currently spreading and
	    infecting new targets, and malware that is only found in
	    online collections (the <em>Zoo</em>):
	  </p>

	  <div class="Math" align="center">
	    M<sub>known</sub> = M<sub>ITW</sub> &cup; M<sub>Zoo</sub>
	  </div>

	  <p>
	    The set <span class="Math">M<sub>ITW</sub></span> represents
	    the collection of malicious programs known to be in the
	    wild, ``spreading as a result of normal day-to-day
	    operations on and between the computers of unsuspecting
	    users'' [<a href="#wildlist">35</a>]. The set <span
	      class="Math">M<sub>Zoo</sub></span> contains known malware
	    that is not currently known to be in the wild, either
	    because it is no longer infecting systems or because it is a
	    lab sample that never spread. In 1995, Joe Wells started
	    <em>The WildList</em>, a list of viruses reported to be in
	    the wild over a certain period (currently it is updated
	    monthly [<a href="#wildlist">35</a>]). <em>The WildList</em>
	    is an important test set for malware detectors, and
	    established testing and certifications labs (e.g. ICSA [<a
	      href="#icsa-certification">17</a>], Checkmark [<a
	      href="#checkmark-certification-av1">38</a>,<a
	      href="#checkmark-certification-av2">39</a>], and Virus
	    Bulletin [<a href="#vb-certification">36</a>]) require
	    malware detectors to identify all programs in <span
	      class="Math">M<sub>ITW</sub></span> with a detection rate of
	    100% (based on a current version of <em>The WildList</em>)
	    and at least 90% for malware in <span
	      class="Math">M<sub>Zoo</sub></span>. However, using <span
	      class="Math">M<sub>ITW</sub></span> as a test set does not
	    evaluate the resilience of malware detectors to obfuscation
	    transformation, a common strategy used by blackhats.
	  </p>

	  <p>
	    Marx [<a href="#marx2000-avtesting">23</a>] presented a
	    compendium of anti-virus program testing techniques and
	    methods, with descriptions of the relevant metrics and the
	    possible pitfalls that can invalidate the results. The
	    question of assessing the unknown malware detection
	    capabilities is still open, and Brunnstein [<a
	      href="#brunnstein2002-heureka2">2</a>] performed tests
	    geared specifically at the heuristic detection algorithms
	    that many malware detectors employ. Marx [<a
	      href="#marx2002-avretrotest">24</a>] extended this work by
	    proposing the use of <em>retrospective testing</em> to
	    measure the quality of the heuristics. Retrospective testing
	    executes older versions of virus scanners against new
	    malware not known at the time the virus scanners were
	    released. To our knowledge, ours is the first paper that
	    uses program obfuscation as a testing technique for malware
	    detectors.  Moreover, signature extraction has also not been
	    addressed in the existing literature.
	  </p>

	</div>

	<!-- ................................................................... -->

	<div class="SubSection">
	  <a name="section_2.2_related_software_testing"></a>
	  <div class="Title">2.2. Related work on software testing</div>

	  <p>
	    The software testing literature is rich in models and
	    approaches. When choosing a testing methodology, one has to
	    consider the nature of malware detectors: first, most
	    malware detectors are commercial software, and their
	    algorithms and technologies are proprietary. This limits us
	    to the use of functional or black-box testing [<a
	      href="#myers1979-testing">29</a>]. Second, the very nature
	    of malware makes it impossible to define it clearly: a
	    malware can be seen as code that bypasses the security
	    policy of a system. This means that the input space cannot
	    be simply reduced to a manageable size by using equivalence
	    classes, such as ``worms,'' ``viruses,'' and ``benign
	    programs''. We consider two methods for exploring the input
	    space: random and adaptive testing.
	  </p>

	  <p>
	    <em>Random testing</em> generates tests by randomly selecting
	    inputs from the domain of a given program. While intuitively
	    considered a poor strategy for test-case generation, random
	    testing is regarded by many as a viable approach. Compared to the
	    general strategy of <em>partition testing</em> [<a
	    href="#ntafos1998-randompartition">30</a>], which describes any
	    approach to use information about the program to split the input
	    domain into partitions and to generate test cases from each
	    partition, random testing can perform as well [<a
	    href="#duranntafos1984-partition">10</a>] or better [<a
	    href="#hamlettaylor1990-partition">14</a>]. More recent work [<a
	    href="#chenyu1994-partition">3</a>,<a
	    href="#ntafos2001-partition">31</a>,<a
	    href="#weyukerjeng1991-partition">40</a>] proves that under
	    certain conditions (if, for example, a partition contains only
	    correct, non-fault inducing, inputs) random testing is a good
	    approach, while partition testing is better under different
	    scenarios (for example, when all the partitions have equal
	    size). <em>Operational testing</em> [<a
	    href="#franklhamletlittlewoodstrigini1997-reliabilitytesting">12</a>],
	    a variant of random testing, uses probable operational input
	    (i.e. typical usage data) to generate test sets. In practice,
	    <em>fuzz testing</em> [<a
	    href="#forrestermiller2000-ntfuzz">11</a>,<a
	    href="#millerfredriksenso1990-unixfuzz">27</a>,<a
	    href="#millerkoskileemagantymurthynatarajansteidl1995-x11fuzz">28</a>],
	    a form of random testing, has proved to be a powerful technique
	    for generating test sets for programs.  To our knowledge, the
	    combination of program obfuscation and random testing for
	    evaluating malware detectors has not been investigated before.
	  </p>

	  <p>
	    <em>Adaptive testing</em>, introduced by Cooper [<a
	    href="#cooper1976-adaptive">8</a>], describes a testing
	    strategy that uses an objective function to guide the
	    exploration of the input domain. By iteratively generating
	    test sets based on evaluation of an objective function
	    over the previous testing results, adaptive testing can
	    identify the performance boundary of a given program. At
	    each iteration step, the test set is generated by
	    perturbing the previous set according to some algorithm
	    that attempts to optimize (minimize or maximize) an
	    objective function. Our signature-extraction algorithm
	    bears some similarity to these adaptive-testing
	    techniques. However, signature extraction has not been
	    addressed in the literature before. The testing strategy
	    described in this paper also combines features of
	    operational testing with those of <em>debug testing</em>
	    [<a
	    href="#franklhamletlittlewoodstrigini1997-reliabilitytesting">12</a>],
	    where tests are designed to seek out failures. In this
	    area, Hildebrandt and Zeller [<a
	    href="#hildebrandtzeller2000-failureinput">15</a>,<a
	    href="#hildebrandtzeller2002-failureinput">16</a>]
	    introduced a delta debugging algorithm that seeks to
	    minimize failure-inducing input and runs in <span
	    class="Math">O(n<sup>2</sup>)</span> time. We enhance
	    their technique by employing knowledge about the input
	    structure (all inputs to the malware detector are valid
	    programs) and by modeling the malware detector, to achieve
	    a running time of <span class="Math">O(k log(n))</span>
	    for the signature-extraction algorithm, where <span
	    class="Math">k</span> is the amount of information we
	    learn about the signature used by the malware detector in
	    one iteration.
	  </p>

	</div>

	<!-- ................................................................... -->

	<div class="SubSection">
	  <a name="section_2.3_related_obfuscation"></a>
	  <div class="Title">2.2. Related work on program obfuscation</div>

	  <p>
	    Obfuscation techniques have been the focus of much research
	    due to their relevance to the protection of intellectual
	    property present in software programs. The goal is to render
	    a program hard to analyze and reverse engineer. Collberg,
	    Thomborson, and Low [<a
	      href="#collbergthomborsonlow1997-taxonomy">5</a>] defined
	    obfuscation as a program transformation that maintains the
	    ``observable behavior.''  They also introduced a taxonomy of
	    obfuscation transformations based on three metrics: potency
	    against a human analyst, resilience against an automatic
	    tool, and execution overhead. Furthermore, they proposed
	    control transformations using <em>opaque predicates</em>
	    (predicates with values known at obfuscation time, but
	    otherwise hard to analyze and statically compute) and
	    computation and data transformations that break known
	    algorithms and data structure abstractions while preserving
	    semantics [<a
	      href="#collbergthomborsonlow1998-breakingabstractions">6</a>,<a
	      href="#collbergthomborsonlow1997-opaque">7</a>]. Chow, Gu,
	    Johnson, and Zakharov [<a
	      href="#chowgujohnsonzakharov01-controlobfuscation">4</a>]
	    presented an obfuscation approach based on inserting hard
	    combinatorial problems with known solutions into the program
	    using semantics-preserving transformations, which the
	    authors claim make deobfuscation P<span style="font-size:
	      smaller;">SPACE</span>-complete.  Wang [<a
	      href="#wang2000-security4survivability">37</a>] introduced
	    obfuscation as ``one-way translation'' in the context of a
	    security architecture for survivability.  Wroblewski's
	    general method of obfuscation [<a
	      href="#wroblewski2002-generalobfuscation">41</a>] uses code
	    reordering and insertion in the context of
	    semantics-preserving obfuscation transformations. To our
	    knowledge, ours is the only paper which considers program
	    obfuscation as a technique for testing malware detectors.
	  </p>

	  <p>
	    While deciding on the initial obfuscation techniques to
	    focus on, we were also influenced by several existing
	    blackhat tools. <em>Mistfall</em> (by <em>z0mbie</em>) is a
	    library for binary obfuscation, specifically written to
	    blend malicious code into a host program [<a
	      href="#z0mbie-mistfall">42</a>].  It can encrypt the virus
	    code and data, obfuscate the virus control flow, and blend
	    it into the host program. <em>Burneye</em> (by
	    <em>TESO</em>) is a Linux binary encapsulation tool that
	    encrypts a binary (possibly multiple times) and packages it
	    into a new binary with an extraction tool [<a
	      href="#teso-burneye">34</a>].
	  </p>

	</div>

      </div>

      <!-- ................................................................... -->
      <!-- ................................................................... -->
      <!-- ................................................................... -->

      <div class="Section">
	<a name="sec:obfuscation"></a>
	<div class="Title">3. Program Obfuscation</div>

	<p>
	  This section provides a definition of program obfuscation and
	  our implementation of an obfuscator for Visual Basic.
	</p>

	<p>
	  An obfuscation transformation <span
	    class="Math">&sigma;</span> is a program transformation that
	  does not change the behavior of the original program, i.e., it
	  adds code or transforms the existing code in such a way that
	  it does not affect the overall result, or it adds new
	  behaviors without affecting the existing ones. If we regard a
	  program as a function that maps inputs to outputs, <span
	    class="Math">p : Inputs &rarr; Outputs</span>, then an
	  obfuscation transformation <span class="Math">&sigma;</span>
	  must conform to the following condition:
	</p>

	<div class="Math" align="center">
	  &forall; I &isin; Inputs . p(I) &sube; (&sigma;(p))(I)
	</div>

	<p>
	  Notice that for a specific input <span class="math">I</span>
	  the obfuscated program <span class="Math">&sigma;(p)</span>
	  allows a larger set of outputs than the original program <span
	    class="Math">p</span>.  This relaxed definition of obfuscation
	  (as opposed to a strict semantics-preserving definition) is
	  similar to the one introduced in [<a
	    href="#wang2000-security4survivability">37</a>] and has
	  several key benefits in our context. First, when used for
	  generating test cases, this relaxed definition models the
	  evolution of actual malicious code better. This is based on
	  the observation that new malware borrows from existing code,
	  and many times successive versions of the same malware only
	  add new behaviors. Second, in the case of viruses that infect
	  executable binary files, we want to treat the program hosting
	  the virus as a decoy, i.e., as an obfuscation that the virus
	  uses to hide itself.
	</p>

	<!-- ................................................................... -->

	<div class="SubSection">
	  <a name="section_3.1_implementation"></a>
	  <div class="Title">3.1. Our Implementation</div>

	  <p>
	    Our obfuscator works on Visual Basic programs. First, we
	    parse the program and construct various structures which are
	    used by the obfuscation transformations.  We chose three
	    program structures as the bases for applying these
	    transformations: the <em>abstract syntax tree (AST)</em>
	    built from parsing the original program, the <em>control
	      flow graph (CFG)</em> for each subroutine (procedure or
	    function) in the program, and the list of subroutines. Other
	    structures, such as call graphs, data dependence graphs,
	    control dependence graphs, and system dependence graphs,
	    are constructed on demand from these three data structures.
	  </p>

	  <p>
	    Transformations applied to the AST allow a detailed
	    specification of the program layout on disk, where the order
	    of instructions might be different from the execution order.
	    Transformations applied to the CFG are defined as graph
	    transformations, using node replacement grammars with a
	    limited amount of context sensitivity provided by evaluating
	    the truth value of static analysis predicates associated to
	    each rewrite rule.  Transformations applied to the list of
	    subroutines relate to adding, removing, or replacing
	    subroutines in the program. This type of transformation is
	    in most cases applied in combination with CFG or AST
	    transformations. For example, outlining (extracting code
	    from an existing subroutine and creating a new subroutine
	    with this code) involves both a set of CFG transformations
	    and a subroutine addition.
	  </p>

	  <p>
	    Next, we describe four important obfuscations supported by
	    our implementation. These obfuscations are inspired by
	    existing real-world malware.
	  </p>

	</div>

	<!-- ................................................................... -->

	<div class="SubSection">
	  <a name="section_3.2_samples"></a>
	  <div class="Title">3.2. Sample obfuscations</div>

	  <p>
	    We present four obfuscation transformations we
	    implemented. Each program transformation is defined by its
	    type and its associated parameters.  The code example used
	    in this section is derived from a fragment of the Homepage
	    virus shown in <a href="#l:homepage-original">Listing
	      1</a>. The code in this listing implements the initial steps
	    of the replication algorithm in the Homepage virus, by
	    loading a copy of itself into a memory buffer and getting a
	    handle to the system temporary directory.
	  </p>

	  <table class="Listing" align="center" cellpadding="0" cellspacing="0" border="0">
	    <tbody>
	      <tr>
		<td>
		  <a name="l:homepage-original"></a>
		  <pre>
On Error Resume Next
Set WS = CreateObject("WScript.Shell")
Set FSO= Createobject("scripting.filesystemobject")
Folder=FSO.GetSpecialFolder(2)

Set InF=FSO.OpenTextFile(WScript.ScriptFullname,1)
Do While InF.AtEndOfStream<>True
  ScriptBuffer=ScriptBuffer&InF.ReadLine&vbcrlf
Loop</pre>
		  <div class="Caption">
		    <span class="Label">Listing 1.</span> Fragment of
		    the Homepage virus, used in illustrating obfuscation
		    transformations henceforth.
		  </div>
		</td>
	      </tr>
	    </tbody>
	  </table>

	  <div class="SubSubSection">
	    <a name="section_3.2.1_garbage"></a>
	    <div class="Title">3.2.1. Garbage insertion</div>

	    <p>
	      Garbage insertion adds sequences which do not modify the
	      behavior of the program. This insertion can be done
	      anywhere in the code section of the program.
	    </p>

	    <p>
	      <b>Parameters</b>. The following parameters control this
	      obfuscation transformation:
	    </p>

	    <ul>
	      <li>
		<b>location <span class="Math">&lambda;</span></b>
		represents the program point where the garbage insertion
		is to be performed;
	      </li>
	      <li>
		<b>size <span class="Math">&sigma;</span></b> represents
		the size of the garbage code sequence to be generated
		for insertion.
	      </li>
	    </ul>

	    <p>
	      Currently our implementation supports insertion of garbage
	      code at any location in the program. The garbage code
	      sequence is randomly generated from a combination of
	      assignments (involving simple arithmetic) and branch
	      statements with conditionals based on variables used only
	      in the garbage code.
	    </p>

	  </div>

	  <div class="SubSubSection">
	    <a name="sec:obf-reorder"></a>
	    <div class="Title">3.2.2. Code Reordering</div>

	    <p>
	      The code reordering obfuscation changes the order of
	      program instructions.  The physical order is changed while
	      maintaining the original execution order through the use
	      of control-flow instructions (branches and
	      jumps). Branches are inserted with conditionals defined
	      and computed such that the branch is always taken. The
	      conditional expression can be based on a complex
	      computation.
	    </p>

	    <p>
	      The execution order of instructions can be changed only if
	      the program behavior is not affected. Independent
	      consecutive instructions (without any dependencies between
	      them) can thus be interchanged.
	    </p>

	    <p>
	      <b>Parameters</b>. The following parameters control the
	      code-reordering obfuscation:
	    </p>

	    <ul>
	      <li>
		<b>program range <span
		    class="Math">(&lambda;<sub>begin</sub>,
		    &lambda;<sub>end</sub>)</span></b> represents the set of
		instructions to be reordered;
	      </li>
	      <li>
		<b>type of reordering <span class="Math">&tau;</span></b>
		selects between physical reordering, execution
		reordering, or both.
	      </li>
	    </ul>

	    <p>
	      The reorder obfuscation transformation can process any
	      statements in the program and physically reorder them
	      either randomly or in an order strictly reversed from the
	      original program order. <a
		href="#l:homepage-reorder">Listing 2</a> shows an example
	      of a reordering creating a strictly reversed order.
	    </p>

	    <table class="Listing" align="center" cellpadding="0" cellspacing="0" border="0">
	      <tbody>
		<tr>
		  <td>
		    <a name="l:homepage-reorder"></a>
		    <pre>
GoTo label_0001
label_0006:
  Do While InF.AtEndOfStream<>True
    ScriptBuffer=ScriptBuffer&InF.ReadLine&vbcrlf
  Loop
  GoTo label_0007
label_0005:
  Set InF=FSO.OpenTextFile(WScript.ScriptFullname,1)
  GoTo label_0006
label_0004:
  Folder=FSO.GetSpecialFolder(2)
  GoTo label_0005
label_0003:
  Set FSO= Createobject("scripting.filesystemobject")
  GoTo label_0004
label_0002:
  Set WS = CreateObject("WScript.Shell")
  GoTo label_0003
label_0001:
  On Error Resume Next
  GoTo label_0002
label_0007:</pre>
		    <div class="Caption">
		      <span class="Label">Listing 2.</span> The result of a
		      reorder obfuscation applied to the code fragment in
		      <a href="#l:homepage-original">Listing 1</a>.
		    </div>
		  </td>
		</tr>
	      </tbody>
	    </table>

	  </div>

	  <div class="SubSubSection">
	    <a name="sec:obf-rename"></a>
	    <div class="Title">3.2.3. Variable Renaming</div>

	    <p>
	      The renaming obfuscation transformation replaces a given
	      identifier with another. The replacement can occur
	      throughout a given live range of the variable, or
	      throughout the whole program.
	    </p>

	    <p>
	      <b>Parameters</b>. The following parameters control the
	      renaming obfuscation:
	    </p>

	    <ul>
	      <li>
		<b>name <span class="Math">&nu;<sub>old</sub></span></b>
		is the name of the variable to be replaced;
	      </li>
	      <li>
		<b>new name <span
		    class="Math">&nu;<sub>new</sub></span></b> is the new
		identifier meant to replace <span
		  class="Math">&nu;<sub>old</sub></span>;
	      </li>
	      <li>
		<b>program location <span
		    class="Math">&lambda;</span></b> identifies a program
		location part of the live range of <span
		  class="Math">&nu;<sub>old</sub></span> over which the
		replacement is to take place.
	      </li>
	    </ul>

	    <p>
	      Our implementation can rename any variable in the program,
	      either for a limited range of statements or throughout its
	      live range. The new names are randomly picked from the
	      system dictionary, as illustrated in the example shown in
	      <a href="#l:homepage-rename">Listing 3</a>, where all the
	      variables are renamed throughout the program body.
	    </p>

	    <table class="Listing" align="center" cellpadding="0" cellspacing="0" border="0">
	      <tbody>
		<tr>
		  <td>
		    <a name="l:homepage-rename"></a>
		    <pre>
On Error Resume Next
Set inquiries = CreateObject("WScript.Shell")
Set will= Createobject("scripting.filesystemobject")
grimier=will.GetSpecialFolder(2)

Set rumour=will.OpenTextFile(WScript.ScriptFullname,1)
Do While rumour.AtEndOfStream<>True
  optimizers=optimizers&rumour.ReadLine&vbcrlf
Loop</pre>
		    <div class="Caption">
		      <span class="Label">Listing 3.</span> The result
		      of a variable renaming obfuscation applied to the
		      code fragment in <a
			href="#l:homepage-original">Listing 1</a>.
		    </div>
		  </td>
		</tr>
	      </tbody>
	    </table>

	  </div>

	  <div class="SubSubSection">
	    <a name="sec:obf-encapsulate"></a>
	    <div class="Title">3.2.4. Code and Data Encapsulation</div>

	    <p>
	      The encapsulation obfuscation is similar to a
	      self-extracting file archive. The chosen program
	      fragment is encoded using a specified algorithm, and a
	      decoding procedure is inserted in the program. At
	      execution time, when the encapsulated program fragment is
	      needed, the decoding procedure is executed and the
	      original program fragment is used, i.e., executed if it is
	      a code portion, or accessed if it is a data portion. This
	      obfuscation is designed to be used with run-time immutable
	      program portions (e.g. code of non-self-modifying
	      programs, and read-only data).
	    </p>

	    <p>
	      <b>Parameters</b>. The following parameters control the
	      encapsulation obfuscation transformation:
	    </p>

	    <ul>
	      <li>
		<b>program range <span
		    class="Math">(&lambda;<sub>begin</sub>,
		    &lambda;<sub>end</sub>)</span></b> represents the
		program portion to be encapsulated;
	      </li>
	      <li>
		<b>type of encapsulation <span
		    class="Math">&tau;</span></b> specifies the technique
		used to transform the program range;
	      </li>
	      <li>
		other parameters specific to the type of encapsulation.
	      </li>
	    </ul>

	    <p>
	      The type of encapsulation <span class="Math">&tau;</span>
	      selects the algorithm used to transform the sequence of
	      bits representing the program fragment. The type of
	      encapsulation can range from simple uuencoding, to any
	      compression technique (e.g. ZIP), and to any encryption
	      technique (e.g. RSA).
	    </p>

	    <p>
	      The encapsulation obfuscator currently offers two types of
	      encodings: the <em>identity encoding</em> and a <em>hex
		encoding</em>. The identity encoding converts the code to
	      a string and wraps it with a call to an interpreter (in
	      Visual Basic, this is achieved using the
	      <tt>Execute()</tt> function). A hex encoding replaces each
	      byte in the original program portion with its ASCII
	      representation (in hexadecimal) and passes the resulting
	      string to the interpreter. The implementation is modular,
	      allowing for additional encodings to be easily plugged
	      in. A sample of the hex encoding is shown in <a
		href="#l:homepage-encode">Listing 4</a>.
	    </p>

	    <table class="Listing" align="center" cellpadding="0" cellspacing="0" border="0">
	      <tbody>
		<tr>
		  <td>
		    <a name="l:homepage-encode"></a>
		    <pre>
Execute( hex_decode( "4F6E204572726F7220526573" &
                     "756D65204E6578740A536574" &
                     ...
                     "66657226496E462E52656164" &
                     "4C696E6526766263726C660A" &
                     "4C6F6F700A" ) )

Function hex_decode( S )
  HexDecoder=""
  For I = 1 To Len( S ) Step 2
    dOne = CInt( "&H" & Mid( S, I, 1 ) )
    dTwo = CInt( "&H" & Mid( S, I + 1, 1 ) )
    hex_decode = hex_decode&Chr( dOne * 16 + dTwo )
  Next
End Function</pre>
		    <div class="Caption">
		      <span class="Label">Listing 4.</span> The result
		      of a hex encoding obfuscation applied to the code
		      fragment in <a href="#l:homepage-original">Listing
			1</a>.
		    </div>
		  </td>
		</tr>
	      </tbody>
	    </table>

	  </div>

	</div>

      </div>

      <!-- ................................................................... -->
      <!-- ................................................................... -->
      <!-- ................................................................... -->

      <div class="Section">
	<a name="sec:methodology"></a>
	<div class="Title">4. Testing methodology</div>

	<p>
	  A malware detector <span class="Math">D</span> works by
	  analyzing a data object (a file, an email message, or a
	  network packet) and determining whether the data contains an
	  executable and whether the executable is malicious. The first
	  test performed by the malware detector (to determine the
	  presence of executable content) is usually based on the host
	  operating system's method for discovering the type of the
	  data. The type can be determined based on MIME type headers,
	  file extensions, or a ``magic number'' that is unique to a
	  file format. Given that techniques exist to determine whether
	  a data object contains an executable, we restrict our
	  definition of a malware detector to admit only executable
	  programs as inputs.
	</p>

	<p>
	  We define a malware detector <span class="Math">D</span> as a
	  function whose domain and range are the set of executable
	  programs <span class="Math">P</span> and the set <span
	    class="Math">{ malicious, benign }</span>, respectively. In
	  other words, a malware detector <span class="Math">D</span> is
	  a function <span class="Math">D : P &rarr; { malicious, benign
	    }</span> defined as:
	</p>

	<table align="center">
	  <tbody>
	    <tr>
	      <td rowspan="2">
		<span class="Math">D(p)</span> =
	      </td>
	      <td rowspan="2"><span style="position: relative; top: -0.1em; font-size: 4em;">{</span></td>
	      <td>
		<span class="Math">malicious</span>
	      </td>
	      <td style="padding-left: 3em;">
		if <span class="Math">p</span> contains malicious code
	      </td>
	    </tr>
	    <tr>
	      <td>
		<span class="Math">benign</span>
	      </td>
	      <td style="padding-left: 3em; padding-bottom: 0.5em;">
		otherwise
	      </td>
	    </tr>
	  </tbody>
	</table>

	<p>
	  Testing a detector <span class="math">D</span> means iterating
	  over all input programs <span class="Math">p &isin; <span
	      class="Calligraphic">P</span></span> and checking the
	  correctness of the answer. In this context, <em>false
	    positives</em> are benign programs that the malware detection
	  tool marks as infected. <em>False negatives</em> are malwares
	  that the detection tool fails to recognize. Conversely, the
	  <em>hit rate</em> measures the ratio of malicious programs
	  detected out of the malwares used for testing. In testing a
	  malware detector, the goal is to verify whether the tool
	  detects all malware. Given the potential threat posed by
	  malware, it is critical to reduce the number of false
	  negatives to be as close to zero as possible, since each false
	  negative represents an undetected malicious program that is
	  loose in the system. On the other hand, the number of false
	  positives is important in determining the usability of the
	  malware detector: if it incorrectly flags too many benign
	  programs as infected, the user may lose faith in the malware
	  detector and stop using it altogether.
	</p>

	<p>
	  Since the set <span class="Math">P</span> of all possible
	  programs is infinite, simply enumerating all inputs for the
	  malware detector is not possible. Every test set is thus
	  finite, and the false positive, false negative, and hit rates
	  are defined with respect to a given test set. A test set <span
	    class="Math"><span
	      class="Calligraphic">P</span><sub>T</sub></span> is classified
	  into two disjoint sets, one of benign programs <span
	    class="Math"><span class="Calligraphic">B</span></span> and
	  one of malicious programs <span class="Math"><span
	      class="Calligraphic">M</span></span>. The false positive rate
	  <span class="Math">FP<sub><span
		class="Calligraphic">P</span><sub>T</sub></sub></span>, false
	  negative rate <span class="Math">FN<sub><span
		class="Calligraphic">P</span><sub>T</sub></sub></span>, and
	  hit rate <span class="Math">HR<sub><span
		class="Calligraphic">P</span><sub>T</sub></sub></span> (all
	  relative to the test set <span class="Math"><span
	      class="Calligraphic">P</span><sub>T</sub></span>) are defined
	  as follows:
	</p>

	<table class="Math" align="center">
	  <tbody>

	    <tr>
	      <td rowspan="2" style="padding-right: 1em;">
		FP<sub><span class="Calligraphic">P</span><sub>T</sub></sub>
	      </td>
	      <td rowspan="2" style="padding-right: 1em;">
		=
	      </td>
	      <td style="text-align: center; border-bottom: 1px solid black; padding-bottom: 0.5em;">
		{ p &isin; <span class="Calligraphic">B</span> : D(p) = <em>malicious</em> }
	      </td>
	    </tr>
	    <tr>
	      <td style="text-align: center; padding-bottom: 0.5em;">
		| <span class="Calligraphic">B</span> |
	      </td>
	    </tr>

	    <tr>
	      <td rowspan="2" style="padding-right: 1em;">
		FN<sub><span class="Calligraphic">P</span><sub>T</sub></sub>
	      </td>
	      <td rowspan="2" style="padding-right: 1em;">
		=
	      </td>
	      <td style="text-align: center; border-bottom: 1px solid black; padding-bottom: 0.5em;">
		{ p &isin; <span class="Calligraphic">M</span> : D(p) = <em>benign</em> }
	      </td>
	    </tr>
	    <tr>
	      <td style="text-align: center; padding-bottom: 0.5em;">
		| <span class="Calligraphic">M</span> |
	      </td>
	    </tr>

	    <tr>
	      <td rowspan="2" style="padding-right: 1em;">
		HR<sub><span class="Calligraphic">P</span><sub>T</sub></sub>
	      </td>
	      <td rowspan="2" style="padding-right: 1em;">
		=
	      </td>
	      <td style="text-align: center; border-bottom: 1px solid black; padding-bottom: 0.5em;">
		{ p &isin; <span class="Calligraphic">M</span> : D(p) = <em>malicious</em> }
	      </td>
	    </tr>
	    <tr>
	      <td style="text-align: center; padding-bottom: 0.5em;">
		| <span class="Calligraphic">M</span> |
	      </td>
	    </tr>

	  </tbody>
	</table>

	<p>
	  The goal of the testing process is to measure a malware
	  detector's false positive, false negative, and hit rates for a
	  test set <span class="Math"><span
	      class="Calligraphic">P</span><sub>T</sub></span> and provide a
	  measure of the detection efficacy. As with any other testing
	  procedure, the final assessment of the quality of a malware
	  detector depends on the quality of the test set and the
	  metrics that reflect the behavior of the program against the
	  test inputs.  Various testing techniques for evaluating
	  malware detectors differ in their method for generating the
	  test set <span class="Math"><span
	      class="Calligraphic">P</span><sub>T</sub></span>. We describe
	  a technique for generating tests for malware detectors which
	  is based on program obfuscation.
	</p>

	<div class="SubSection">
	  <a name=""></a>
	  <div class="Title">4.1. Generating tests using program obfuscation</div>

	  <p>
	    For our testing effort, we focus on malware. Therefore, we
	    only report the false negative and hit rate of various
	    malware detectors.  We use obfuscation transformations
	    applied to known malware to obtain a large number of new
	    test programs that are semantically equivalent to the
	    original malware - thus, the new test samples are just as
	    malicious as the original malware and should be flagged by
	    the detector.  In other words, our test set <span
	      class="Math"><span
		class="Calligraphic">P</span><sub>T</sub></span> is created
	    by applying semantics-preserving obfuscation transformations
	    to a set of known malware. Our testing technique is geared
	    towards evaluating the resilience of malware detectors to
	    obfuscation transformations. This testing technique answers
	    question 1 raised in the introduction.
	  </p>

	  <p>
	    The testing proceeded as follows: we collected <span
	      class="Math">8</span> viruses and checked them against <span
	      class="Math">3</span> commercial virus scanners. We made
	    sure all the viruses were active and detected in their
	    original form by these commercial virus scanners. To obtain
	    our test set, we applied obfuscations from our
	    implementation to the <span class="Math">8</span> viruses
	    (see <a href="#sec:obfuscation">Section 3</a> for
	    description of the obfuscation transformations). The
	    parameters for each obfuscation were randomly varied to
	    obtain multiple variants of each virus (unless the number of
	    variants was naturally limited, e.g. renaming is constrained
	    by the number of variables in the program). This approach
	    follows the random testing technique, as we sample the space
	    of obfuscated variants, instead of extensively enumerating
	    all possible variants for each virus and each obfuscation -
	    time consuming, and impossible in some cases. The number of
	    variants generated for each virus were between 31 and
	    5,592. As our interest is in evaluating multiple malware
	    detectors, we use the same test set for each
	    detector. Results of our testing appears in
	    <a name="#subsec:experiments-resilience">Section 5.1</a>.
	  </p>

	  <p>
	    The architecture of our testing toolkit is shown in <a
	      href="#f:overall_arch">Figure 1</a>. The <em>obfuscation
	      engine</em> applies obfuscation transformations according to
	    specified parameters. The <em>result analyzer</em> records
	    the output of the malware detector being tested. The
	    <em>obfuscation parameter generator</em> determines the next
	    set of parameters.
	  </p>

	  <table class="FigureFloat" align="center">
	    <tbody>
	      <tr>
		<td>
		  <a name="f:overall_arch"></a>
		  <center><img style="padding: 1em;" src="overall_architecture_1.png"></center>
		  <div class="Caption">
		    <span class="Label">Figure 1. </span> The
		    architecture of the malicious code detector test
		    toolkit.
		  </div>
		</td>
	      </tr>
	    </tbody>
	  </table>

	</div>

	<div class="SubSection">
	  <a name="sec:signature-extraction"></a>
	  <div class="Title">4.2. Signature extraction</div>

	  <p>
	    We now address the second question from the introduction,
	    i.e., using limitations of a malware detector in handling
	    obfuscations, can a blackhat determine the detection
	    algorithm?  Assume that the malware detector uses a
	    signature to detect malware (this is true of most commercial
	    virus scanners), i.e., a malware is identified by a sequence
	    of statements called the <em>signature</em>.  We introduce
	    an iterative algorithm for discovering the signature used by
	    a virus scanner to detect a specific malicious code. Our
	    algorithm uses the malware detector as a black box and
	    iteratively inputs obfuscated variants of a malware to the
	    detector.  We exploit the fact that most commercial virus
	    scanners are not resistant to the encapsulation obfuscation
	    transformation (see <a href="#sec:obf-encapsulate">Section
	      3.2.4</a> for a description of the encapsulation
	    transformation). We evaluated the three commercial virus
	    scanners against our signature-extraction algorithm. Details
	    of our evaluation can be found in <a
	      href="#subsec:experiments-signature">Section
	      5.2</a>. However, the following conclusions can be drawn
	    from our evaluation:
	  </p>

	  <ul>
	    <li>
	      Weakness in handling obfuscations (in this case
	      encapsulation) can be successfully used by a blackhat to
	      extract signatures used for detection.
	    </li>

	    <li>
	      Most commercial virus scanners are highly susceptible to
	      our signature-extraction algorithm. Using our algorithm we
	      were able to easily extract signatures.
	    </li>
	  </ul>

	  <p>
	    Next, we provide details of our signature-extraction
	    algorithm. We denote by <span class="Math">E<span
		style="font-size: smaller;">NCODE</span>( P,
	      {&lambda;<sub>1</sub>, ..., &lambda;<sub>k</sub>} )</span>
	    the encapsulation obfuscation using the hex encoding applied
	    to <span class="Math">k</span> locations <span
	      class="Math">&lambda;<sub>1</sub>, ...,
	      &lambda;<sub>k</sub></span> of a program <span
	      class="Math">P</span>. We assume that the fragment of a
	    malware that is encoded is ``opaque'' to the virus scanner,
	    i.e., if we apply <span class="Math">E<span
		style="font-size: smaller;">NCODE</span>(P,
	      {&lambda;<sub>1</sub>, ..., &lambda;<sub>k</sub>})</span>,
	    then the virus scanner does not know the contents of
	    locations <span class="Math">&lambda;<sub>1</sub>, ...,
	      &lambda;<sub>k</sub></span>. While the signature extraction
	    algorithm presented uses the hex encoding obfuscation
	    transformation, it does not depend on the specific type of
	    obfuscation. The only requirement is that there exists one
	    ``opaque'' obfuscation transformation for each malware
	    detector-malware combination. During our experiments (<a
	      href="#subsec:experiments-signature">Section 5.2</a>), we we
	    were able to create ``opaque'' encapsulation transformations
	    without any difficulty. Suppose we input to the detector
	    <span class="Math">D</span> the malware <span
	      class="Math">M</span> with the encapsulation transformation
	    <span class="Math">E<span style="font-size:
		smaller;">NCODE</span></span> applied to it. In this case,
	    the detector <span class="Math">D</span> will answer
	    <em>malicious</em> if and only if the signature <span
	      class="Math">&Sigma;<sub>M,D</sub></span> does not overlap
	    with the encapsulated fragment <span
	      class="Math">&lambda;<sub>1</sub>, ...,
	      &lambda;<sub>k</sub></span>. This fact is used in our
	    signature-extraction algorithm and motivates our definition
	    of a signature.
	  </p>

	  <div class="Definition">
	    <div class="Header">
	      <span class="Label">Definition 1</span>
	      <span class="Title">Malware signature</span>
	    </div>

	    <p>
	      Given a malware sample <span class="Math">M = &lang;
		m<sub>1</sub>, ... , m<sub>n</sub> &rang;</span> of <span
		class="Math">n</span> instructions and a malware detector
	      <span class="Math">D</span>, a signature <span
		class="Math">&Sigma;<sub>M,D</sub></span> represents the
	      minimal set of instructions such that:
	    </p>

	    <table class="Math">
	      <tbody>
		<tr>
		  <td rowspan="2">
		    D(E<span style="font-size: smaller;">NCODE</span>( M, A )) =
		  </td>
		  <td rowspan="2"><span style="position: relative; top: -0.1em; font-size: 4em;">{</span></td>
		  <td>
		    <em>benign</em>
		  </td>
		  <td>
		    if A &sube; &Sigma;<sub>M,D</sub> &and; A &ne; &empty;
		  </td>
		</tr>
		<tr>
		  <td>
		    <em>malicious</em>
		  </td>
		  <td>
		    otherwise
		  </td>
		</tr>
	      </tbody>
	    </table>

	    <p>
	      where <span class="Math">A = {&lambda;<sub>1</sub>, ...,
		&lambda;<sub>k</sub>}</span> is the set of program
	      locations encoded.
	    </p>
	  </div>

	  <p>
	    The signature-extraction problem can be stated as follows:
	  </p>

	  <div class="Definition">
	    <div class="Header">
	      <span class="Label">Definition 2</span>
	      <span class="Title">The signature-extraction problem</span>
	    </div>

	    <p>
	      Given a malware sample <span class="Math">M</span> and
	      black-box access to a detector <span
		class="Math">D</span>, find the malware signature <span
		class="Math">&Sigma;<sub>M,D</sub></span> used by <span
		class="Math">D</span> to identify <span
		class="Math">M</span>.
	    </p>
	  </div>

	  <p>
	    <a href="#algo:single-sig">Algorithm 1</a> describes our
	    signature-extraction procedure. The core of the algorithm
	    consists of a repeated binary search over the malware, at
	    each step identifying the outermost (left and right)
	    signature components. At the next iteration step, the binary
	    search continues in the program range defined exclusively by
	    the outermost signature components. After the first
	    iteration of the <b>while</b> loop <span
	      class="Math">L<sub>1</sub></span> and <span
	      class="Math">R<sub>1</sub></span> are the lowest and the
	    highest index of the signature <span
	      class="Math">&Sigma;<sub>M,D</sub></span> in the malware
	    <span class="Math">M = &lang;
	      m<sub>1</sub>,...,m<sub>k</sub> &rang;</span>.  The next
	    iteration of the <b>while</b> loop restricts the search to
	    the fragment <span class="Math">&lang;
	      m<sub>L<sub>i</sub>+1</sub>,..., m<sub>R<sub>i</sub>-1</sub>
	      &rang;</span> of the malware <span class="Math">M</span>.
	  </p>

	  <p>
	    The procedure <span style="font-variant:
	      small-caps;">FindLeftmost</span> finds the leftmost index of
	    the signature in the malware, as illustrated in <a
	      name="#algo:leftmostsig">Algorithm 2</a>. The notation
	    <span class="Math"><span
		class="Calligraphic">O</span>(V)</span> represents a query
	    to the malware detector on <span class="Math">V</span> that
	    returns the name of the detected malware (if any), and <span
	      class="Math">E<span style="font-size:
		smaller;">NCODE</span>( M, L, R )</span> is the
	    encapsulation obfuscation transformation using hex encoding
	    applied to the range <span class="Math">[L ...  R]</span> of
	    the program <span class="Math">M</span>. The procedure <span
	      style="font-variant: small-caps;">FindRightmost</span>
	    (which finds the rightmost index of the signature) is
	    similar, with a search biased towards the right half of the
	    search range.
	  </p>

	  <table class="Algorithm" align="center" cellpadding="0" cellspacing="0" border="0">
	    <tbody>
	      <tr>
		<td>
		  <a name="algo:single-sig"></a>
		  
		  <div class="Body">
		    <div>
		      <b>Input</b>: A malware sample <span
			class="Math">M = &lang; m<sub>1</sub>, ...,
			m<sub>n</sub> &rang;</span> with <span
			class="Math">n</span> instructions, a malware
		      detector <span class="Math">D</span>, and a
		      malware name <span class="Math">&sigma;</span>.
		    </div>

		    <div>
		      <b>Output</b>: The signature given as a set of
		      malware instructions <span
			class="Math">{m<sub>k<sub>0</sub></sub>, ...,
			m<sub>k<sub>i</sub></sub>, ...  }<sub>1 &le;
			  k<sub>i</sub> &le; n</sub></span>.
		    </div>

		    <div class="Declaration">
		      <span class="AlgoName">SignatureExtraction</span>
		      (
		      <span class="Argument">M</span>,
		      <span class="Argument">D</span>,
		      <span class="Argument">&sigma;</span>
		      )
		    </div>

		    <div><b>begin</b></div>
		    <div class="AlgoScope">
		      <div class="AlgoLine">
			<span class="Math">L<sub>0</sub> &larr; 0</span>
		      </div>

		      <div class="AlgoLine">
			<span class="Math">R<sub>0</sub> &larr; N+1</span>
		      </div>

		      <div class="AlgoLine">
			<span class="Math">i &larr; 1</span>
		      </div>

		      <div><b>while</b> <span class="Math">L<sub>i-1</sub> &ne; 0 &and; R<sub>i-1</sub> &ne; 0</span> <b>do</b></div>
		      <div class="AlgoScope">
			<div class="AlgoLine">
			  <span class="Math">L<sub>i</sub> &larr; F<span style="font-size: smaller;">IND</span>L<span style="font-size: smaller;">EFTMOST</span>( M, L<sub>i-1</sub> + 1, R<sub>i-1</sub> - 1, &sigma; )</span>
			</div>

			<div class="AlgoLine">
			  <span class="Math">R<sub>i</sub> &larr; F<span style="font-size: smaller;">IND</span>R<span style="font-size: smaller;">IGHTMOST</span>( M, L<sub>i-1</sub> + 1, R<sub>i-1</sub> - 1, &sigma; )</span>
			</div>

			<div class="AlgoLine">
			  <span class="Math">i &larr; i + 1</span>
			</div>
		      </div>
		      <div><b>end</b></div>

		      <div>
			<b>return</b> <span class="Math">{ m<sub>k</sub> : &exist; j &lt; i . k = L<sub>j</sub> &or; k = R<sub>j</sub> }</span>
		      </div>
		    </div>
		    <div><b>end</b></div>
		  </div>

		  <div class="Caption">
		    <span class="Label">Algorithm 1.</span> Algorithm to
		    extract the signature used by <span
		      class="Math">D</span> to detect <span
		      class="Math">M</span> as <span
		      class="Math">&sigma;</span>.
		  </div>

		</td>
	      </tr>
	    </tbody>
	  </table>

	  <p>
	    If we denote by <span class="Math">k</span> the size of the
	    malware signature (i.e. <span
	      class="Math">|&Sigma;<sub>M,D</sub>|=k</span>) and assume
	    that each query to the malware detector takes unit time,
	    then the running time of our algorithm is <span
	      class="Math">O(k log(n))</span>. During our experiments, we
	    discovered that in most cases <span class="Math">k &lt;&lt;
	      n</span>, which means that the search quickly converges to
	    find a signature. The average-case complexity of our
	    algorithm is significantly better than the <span
	      class="Math">O(k log(n))</span> worst-case
	    complexity. However, due to space limitations, we will not
	    provide the derivation of the average-case complexity.
	  </p>

	  <table class="Algorithm" align="center" cellpadding="0" cellspacing="0" border="0">
	    <tbody>
	      <tr>
		<td>
		  <a name="algo:leftmostsig"></a>

		  <div class="Body">

		    <div>
		      <b>Input</b>: A malware sample <span
			class="Math">M = &lang; m<sub>1</sub>, ...,
			m<sub>n</sub> &rang;</span> with <span
			class="Math">n</span> instructions, a program
		      range <span class="Math">[ L .. R ]</span>, and a
		      malware name <span class="Math">&sigma;</span>.
		    </div>

		    <div>
		      <b>Output</b>: The leftmost index of a signature
		      component within the given range.
		    </div>

		    <div class="Declaration">
		      <span class="AlgoName">FindLeftMost</span>
		      (
		      <span class="Argument">M</span>,
		      <span class="Argument">L</span>,
		      <span class="Argument">R</span>,
		      <span class="Argument">&sigma;</span>
		      )
		    </div>

		    <div><b>begin</b></div>
		    <div class="AlgoScope">
		      <div class="AlgoLine">
			<span class="Math"> <em>sig</em> &larr; 0</span>
		      </div>

		      <div><b>while</b> <span class="Math">L &ne; R</span> </div>
		      <div class="AlgoScope">

			<table class="Math">
			  <tbody>
			    <tr>
			      <td rowspan="2">C &larr;</td>
			      <td style="border-bottom: 1px solid black;">L + R</td>
			    </tr>
			    <tr>
			      <td align="center">2</td>
			    </tr>
			  </tbody>
			</table>

			<div class="AlgoLine">
			  <span class="Math">V &larr; <span style="font-variant: small-caps;">Encode</span>( M, L, C )</span>
			</div>

			<div class="AlgoLine">
			  <b>if</b> <span class="Math"><span class="Calligraphic">O</span>(V) = &sigma;</span> <b>then</b> <span class="Math">L &larr; C</span>
			</div>

			<div class="AlgoLine">
			  <b>else</b> <span class="Math">R &larr; C</span>
			</div>

		      </div>
		      <div><b>end</b></div>

		      <div class="AlgoLine">
			<span class="Math">V &larr; <span style="font-variant: small-caps;">Encode</span>( M, L, R )</span>
		      </div>

		      <div class="AlgoLine">
			<b>if</b> <span class="Math"><span class="Calligraphic">O</span>(V) &ne; &sigma;</span> <b>then</b> <em>sig</em> <span class="Math"> &larr; L</span>
		      </div>

		      <div class="AlgoLine">
			<b>return</b> <em>sig</em>
		      </div>

		    </div>
		    <div><b>end</b></div>

		  </div>

		  <div class="Caption">
		    <span class="Label">Algorithm 2.</span> Algorithm to
		    find the leftmost signature component.
		  </div>

		</td>
	      </tr>
	    </tbody>
	  </table>

	  <p>
	    A malware detector usually uses multiple techniques to
	    identify malicious code. These detection techniques form a
	    hierarchy of signatures, and the detector first searches for
	    the most specific signature and then works its way up the
	    hierarchy. We have extended our algorithm to extract
	    hierarchical signatures. For conciseness, we discuss only
	    results based on the Algorithms <a
	      href="#algo:single-sig">1</a> and <a
	      href="#algo:leftmostsig">2</a>.
	  </p>

	</div>

      </div>

      <!-- ................................................................... -->
      <!-- ................................................................... -->
      <!-- ................................................................... -->

      <div class="Section">
	<a name="sec:experiments"></a>
	<div class="Title">5. Experimental results</div>

	<p>
	  We applied our testing methodology to several Visual Basic
	  malware. Due to space limitations we only report results on 8
	  malwares (Anna Kournikova, Homepage, Melissa, Tune, Chantal,
	  GaScript, Lucky2, and Yovp) that exhibit various infection
	  methods. For detailed descriptions of these malware, we refer
	  the reader to the Symantec Antivirus Research Center's online
	  virus database [<a href="#sarc-virencyclopaedia">32</a>] and
	  the McAfee AVERT Virus Information Library [<a
	    href="#mcafee-avert">25</a>]. We used 3 commercial virus
	  scanners in our tests: Symantec's Norton AntiVirus version
	  7.61.934 for Windows 2000, Network Associates Technology
	  Inc.'s McAfee Virus Scan version 4.24.0 for Linux, and Sophos
	  Plc's Sophos Antivirus version 3.76 (engine version 2.17) for
	  Linux. All the virus scanners had their signature database
	  updated before the tests were performed.
	</p>

	<p>
	  Our testing effort was successful. We gained information about
	  the features of individual detection algorithms (for example,
	  we learned that McAfee can detect malware very well in the
	  presence of renaming obfuscation transformations). Our results
	  suggest directions where improvements are needed. For example,
	  code reordering and encapsulation obfuscations generate much
	  higher false negative rates than renaming and garbage
	  insertion obfuscations. Furthermore, we were able to discover
	  many of the signatures used by the malware detectors and
	  correlate their properties (such as signature size and
	  specificity) with our random testing results.
	</p>

	<p>
	  <b>Obfuscation characteristics</b>.  We investigate the tests
	  generated by randomly applying various obfuscation
	  transformations. <a href="#t:sizes">Table 1</a> lists the
	  original sizes of the malware along with minimum, average and
	  maximum size of their obfuscated variants. In most cases, the
	  size does increase (since we implemented obfuscation
	  transformations that add additional code), but only by a
	  limited amount - this means that in a real world scenario,
	  the variants could spread just as easily as the original
	  viruses, without imposing extra load on the network. There are
	  several cases where the size of the variant is smaller than
	  the size of the original malware. However, this only happens
	  in the case of the renaming obfuscation, i.e., if the new
	  names are shorter than the original names of the variables.
	</p>

	<div class="TableFloat" align="center">
	  <a name="t:sizes"></a>

	  <table border="1" cellspacing="0" cellpadding="0">
	    <thead style="font-style: italic;">
	      <tr>
		<td rowspan="2">Malware name</td>
		<td rowspan="2">Original size</td>
		<td style="text-align: center;" colspan="3">Variant size</td>
	      </tr>
	      <tr style="text-align: center;">
		<td>Min</td>
		<td>Avg</td>
		<td>Max</td>
	      </tr>
	    </thead>
	    <tbody style="text-align: right;">
	      <tr><td style="text-align: left;"><nobr>Anna Kournikova</nobr></td><td>2,824 B</td><td>110.02%</td><td>126.47%</td><td>144.04%</td></tr>
	      <tr><td style="text-align: left;">Homepage</td><td>1,983 B</td><td>118.83%</td><td>156.17%</td><td>193.71%</td></tr>
	      <tr><td style="text-align: left;">Melissa</td><td>4,245 B</td><td>95.64%</td><td>121.81%</td><td>151.39%</td></tr>
	      <tr><td style="text-align: left;">Tune</td><td>7,003 B</td><td>113.13%</td><td>130.77%</td><td>160.47%</td></tr>
	      <tr><td style="text-align: left;">Chantal</td><td>417 B</td><td>119.18%</td><td>222.61%</td><td>291.37%</td></tr>
	      <tr><td style="text-align: left;">GaScript</td><td>3,568 B</td><td>75.28%</td><td>97.25%</td><td>118.61%</td></tr>
	      <tr><td style="text-align: left;">Lucky2</td><td>686 B</td><td>100.00%</td><td>182.94%</td><td>251.31%</td></tr>
	      <tr><td style="text-align: left;">Yovp</td><td>1,223 B</td><td>101.80%</td><td>159.87%</td><td>210.79%</td></tr>
	    </tbody>
	  </table>

	  <div class="Caption">
	    <span class="Label">Table 1.</span> The effect of the
	    obfuscation transformations on malware sizes.
	  </div>
	</div>

	<div class="SubSection">
	  <a name="subsec:experiments-resilience"></a>
	  <div class="Title">5.1. Testing resilience to obfuscations</div>

	  <p>
	    We show the results of our random testing using obfuscated
	    variants in Figures <a href="#fig:false negative rate by obfuscation">2</a>
	    and <a href="#fig:false negative rate by virus">3</a>.
	    Since we only tested on malwares, we do not report the false
	    positive rate. Note that hit rate is simply one minus the
	    false negative rate. The reader is warned against using
	    these results to directly compare the three virus scanners,
	    as they do not represent a complete assessment of their
	    respective strengths and weaknesses. Our intent is to show
	    that our testing techniques can expose enough information to
	    discriminate between these virus scanners. From the results,
	    it is immediately evident that our automatically-generated
	    test sets provide enough data to make judgments of the
	    relative strengths and weaknesses of each scanner with
	    respect to handling obfuscations. For example, from
	    <a href="#fig:false negative rate by obfuscation">Figure 2</a>
	    we can deduce that the McAfee Virus Scanner handles variable
	    renaming very well, while Norton AntiVirus can thwart
	    code-reordering obfuscations.
	  </p>

	  <table class="FigureFloat" align="center">
	    <tbody>
	      <tr>
		<td>
		  <a name="fig:false negative rate by obfuscation"></a>
		  <center><img style="padding: 1em;" src="false_negative_rate_by_obfuscation_1.png"></center>
		  <div class="Caption">
		    <span class="Label">Figure 2. </span> False negative
		    rate for individual obfuscation transformations,
		    averaged over the malware test set.
		  </div>
		</td>
	      </tr>
	    </tbody>
	  </table>

	  <p>
	    It is somewhat surprising that for a given anti-virus tool
	    the false negative rates (and, conversely, the hit rates)
	    vary wildly across the malware set
	    (see <a href="#fig:false negative rate by virus">Figure 3</a>).
	    This leads us to believe that malware are identified using
	    different techniques (some precise, some heuristic) that
	    cope with obfuscations with varying degrees of
	    success. Determining the actual detection techniques
	    requires more data and finer-grained obfuscation techniques
	    and will be investigated in the future.  Another interesting
	    result is that detection results are equally poor for code
	    reordering and encapsulation obfuscation
	    transformations. While encapsulation requires advanced
	    detection mechanisms, such as emulation, sandboxing, and
	    partial evaluation necessary to make the encoded fragments
	    ``visible'', code reordering can be detected through simple
	    traversals of the control-flow graph. Therefore, we can
	    conclude that current anti-virus tools incorrectly assume
	    the order of instructions in the malware to be the same as
	    the execution order.
	  </p>

	  <table class="FigureFloat" align="center">
	    <tbody>
	      <tr>
		<td>
		  <a name="fig:false negative rate by virus"></a>
<!--#if expr="${HTTP_USER_AGENT} = /MSIE/" -->
		  <center><img style="width: 606px; padding-top: 1em; padding-bottom: 1em;" src="false_negative_rate_by_virus_1_ie.png"></center>
<!--#else -->
		  <center><img style="padding-top: 1em; padding-bottom: 1em; position: relative; width: 6.5in;" src="false_negative_rate_by_virus_1.png"></center>
<!--#endif -->
		  <div class="Caption">
		    <span class="Label">Figure 3. False negative rate
		      for individual viruses, averaged over the set of
		      obfuscation transformations.</span>
		  </div>
		</td>
	      </tr>
	    </tbody>
	  </table>

	</div>

	<div class="SubSection">
	  <a name="subsec:experiments-signature"></a>
	  <div class="Title">5.2. Signature extraction results</div>

	  <p>
	    As shown in <a href="#t:avsigs">Table 3</a>, our
	    signature-extraction algorithm was successful in many
	    cases. Due to space limitations we only show the results for
	    four malware samples.  Results of our signature-extraction
	    algorithm on other malware were similar. This proves that an
	    adaptive-testing method, guided by the answers provided
	    through black-box access to the malware detector, is
	    successful in identifying signatures<sup><a
		href="#footnote:2">2</a></sup>.
	  </p>

	  <p>
	    In some cases, the discovered signatures were single
	    statements, while for other detectors the malware signatures
	    consisted of multiple statements. Larger signatures reduce
	    the number of false positives, since there is a smaller
	    chance a benign program will contain the signature, but they
	    are also less resilient to obfuscation attacks (see <a
	      href="#t:sigvsfnr">Table 2</a>). We describe below several
	    facts we learned from our testing results. These results
	    demonstrate the richness of information that can be
	    extracted about malware detectors using adaptive testing
	    techniques.
	  </p>

	  <div class="TableFloat" align="center">
	    <a name="t:sigvsfnr"></a>

	    <table border="1" cellspacing="0" cellpadding="0">
	      <thead style="font-style: italic;">
		<tr>
		  <td rowspan="2">Malware name</td>
		  <td style="text-align: center;" colspan="2">Norton AntiVirus</td>
		  <td style="text-align: center;" colspan="2">Sophos Antivirus</td>
		  <td style="text-align: center;" colspan="2">McAfee Virus Scan</td>
		</tr>
		<tr style="text-align: center;">
		  <td>sig %</td>
		  <td>fn %</td>
		  <td>sig %</td>
		  <td>fn %</td>
		  <td>sig %</td>
		  <td>fn %</td>
		</tr>
	      </thead>
	      <tbody style="text-align: right;">
		<tr><td style="text-align: left;"><nobr>Anna Kournikova</nobr></td><td>3%</td><td>12%</td><td>41%</td><td>12%</td><td>100%</td><td>75%</td></tr>
		<tr><td style="text-align: left;">Melissa</td><td>100%</td><td>87%</td><td>100%</td><td>100%</td><td>23%</td><td>5%</td></tr>
		<tr><td style="text-align: left;">Lucky2</td><td>6%</td><td>85%</td><td>100%</td><td>100%</td><td>22%</td><td>53%</td></tr>
		<tr><td style="text-align: left;">Yovp</td><td>7%</td><td>56%</td><td>100%</td><td>100%</td><td>20%</td><td>38%</td></tr>
	      </tbody>
	    </table>

	    <div class="Caption">
	      <span class="Label">Table 2.</span> The correlation
	      between virus signature size (<em>sig %</em> is the size
	      of the signature relative to the virus text) and the false
	      negative rate (<em>fn %</em>).
	    </div>

	  </div>

	  <p>
	    <b>Signatures vary widely.</b> A quick look through <a
	      href="#t:avsigs">Table 3</a> shows that each anti-virus
	    vendor uses a different signature for a given virus. In some
	    case there are overlaps between signatures from different
	    vendors, e.g., Norton AntiVirus and Sophos Antivirus both
	    refer to the line <tt>Execute e7iqom5JE4z(...)</tt> for the
	    Anna Kournikova virus.
	  </p>

	  <p>
	    <b>Signatures target particular types of malicious
	      activities.</b> Several signatures discovered from our
	    experiments clearly contain code describing a particular
	    trait of the malware. For example, the McAfee Virus Scan
	    signature for Lucky2 contains the code that replicates the
	    virus by creating a copy of itself on the
	    filesystem. Similarly, the Norton AntiVirus signature for
	    Yovp captures the code that replicates the virus through
	    floppy disks.
	  </p>

	  <p>
	    <b>Some signatures cover the whole virus body.</b> When the
	    whole malware body is used as a signature, the
	    signature-extraction algorithm could not identify any
	    individual statements as being part of the signature. Such
	    signatures are most precise in detecting a specific instance
	    of the malware (reducing the false positive rate), but fail
	    to match when the malware code is slightly obfuscated, thus
	    increasing the false negative rate. This is supported in our
	    experimental data by the observed correlation between whole
	    virus body signatures and high false negative rates. For
	    example, our signature extractor indicates that Sophos
	    Antivirus uses the whole virus body as a signature for
	    Melissa, Lucky2, and Yovp. Correspondingly, the false
	    negative rates for these detector-virus pairs are high (100%
	    in <a href="#fig:false negative rate by virus">Figure
	      3</a>).
	  </p>

	  <p>
	    <b>Some signatures are case-sensitive.</b> During the
	    development of our toolkit, we discovered that in certain
	    signatures the case of keywords in Visual Basic appears to
	    be significant. The Microsoft Visual Basic interpreter is
	    case-insensitive with respect to keywords. For certain virus
	    scanners, changing one letter from uppercase to lowercase
	    resulted in the virus not being detected. We intend to
	    further pursue this issue in order to explore the
	    limitations of signature-based virus detectors.
	  </p>

	  <p>
	    <b>Variable renaming handled well.</b> In our tests, the
	    renaming obfuscation transformation did not pose great
	    problems for malware detectors. Specifically, the McAfee
	    Virus Scanner detected almost all variants mutated through
	    variable renaming. When correlating whole virus body
	    signatures with the random testing results, the resilience
	    to variable renaming is the only feature that ameliorated a
	    high false negative rate.
	  </p>

<!--#if expr="${HTTP_USER_AGENT} = /MSIE/" -->
	  <div class="TableFloat" align="center">
<!--#else -->
	  <div class="TableFloat" align="center" style="position: relative; left: -0.5in;">
<!--#endif -->
	    <a name="t:avsigs"></a>

	    <table border="1" cellpadding="0" cellspacing="0">
	      <thead style="font-style: italic;">
		<tr style="text-align: center;">
		  <td>Malware name</td>
		  <td>Malware detector</td>
		  <td>Extracted virus signature</td>
		</tr>
	      </thead>
	      <tbody>

		<tr>
		  <td rowspan="3">Anna Kournikova</td>
		  <td>Norton AntiVirus</td>
		  <td style="padding: 0px;">
		    <pre>
Execute e7iqom5JE4z("X)udQ0VpgjnH...70d2")</pre>
		  </td>
		</tr>
		<tr>
		  <td>Sophos Antivirus</td>
		  <td style="padding: 0px;">
<!--#if expr="${HTTP_USER_AGENT} = /MSIE/" -->
		    <pre style="font-size: smaller;">
<!--#else -->
                    <pre>
<!--#endif -->
<span style="font-size: smaller;"> 1</span> Execute e7iqom5JE4z("X)udQ0VpgjnH...70d2") 
<span style="font-size: smaller;"> 5</span> StTP1MoJ3ZU = Mid( hFeiuKrcoj3, I, 1 ) 
<span style="font-size: smaller;"> 6</span> WHz23rBqlo7 = Mid( hFeiuKrcoj3, I + 1, 1 ) 
<span style="font-size: smaller;"> 8</span> StTP1MoJ3ZU = Chr( 10 ) 
<span style="font-size: smaller;">10</span> StTP1MoJ3ZU = Chr( 13 ) 
<span style="font-size: smaller;">12</span> StTP1MoJ3ZU = Chr( 32 ) 
<span style="font-size: smaller;">14</span> StTP1MoJ3ZU = Chr( Asc( StTP1MoJ3ZU ) - 2 ) 
<span style="font-size: smaller;">18</span> WHz23rBqlo7 = Chr( 10 ) 
<span style="font-size: smaller;">20</span> WHz23rBqlo7 = Chr( 13 ) 
<span style="font-size: smaller;">22</span> WHz23rBqlo7 = Chr( 32 ) 
<span style="font-size: smaller;">24</span> WHz23rBqlo7 = Chr( Asc( WHz23rBqlo7 ) - 2 ) 
<span style="font-size: smaller;">27</span> e7iqom5JE4z = e7iqom5JE4 & WHz23rBqlo7 & StTP1MoJ3ZU</pre>
		  </td>
		</tr>
		<tr>
		  <td>McAfee Virus Scan</td>
		  <td><em>The whole body of the malware.</em></td>
		</tr>

		<tr>
		  <td rowspan="3">Melissa</td>
		  <td>Norton AntiVirus</td>
		  <td><em>The whole body of the malware.</em></td>
		</tr>
		<tr>
		  <td>Sophos Antivirus</td>
		  <td><em>The whole body of the malware.</em></td>
		</tr>
		<tr>
		  <td>McAfee Virus Scan</td>
		  <td><em>23 statements from the malware body.</em></td>
		</tr>

		<tr>
		  <td rowspan="3">Lucky2</td>
		  <td>Norton AntiVirus</td>
		  <td style="padding: 0px;"><pre>FSO.CopyFile Melhacker, target.Name, 1</pre></td>
		</tr>
		<tr>
		  <td>Sophos Antivirus</td>
		  <td><em>The whole body of the malware.</em></td>
		</tr>
		<tr>
		  <td>McAfee Virus Scan</td>
		  <td style="padding: 0px;">
<!--#if expr="${HTTP_USER_AGENT} = /MSIE/" -->
		    <pre style="font-size: smaller;">
<!--#else -->
                    <pre>
<!--#endif -->
<span style="font-size: smaller;"> 1</span> Dim Melhacker, WshShell, FSO, VX, VirusLink
<span style="font-size: smaller;"> 6</span> Melhacker = Wscript.ScriptFullName
<span style="font-size: smaller;"> 7</span> VX = Left( Melhacker, InStrRev( Melhacker, "\" ) )
<span style="font-size: smaller;"> 9</span> FSO.CopyFile Melhacker, target.Name, 1</pre>
		  </td>
		</tr>

		<tr>
		  <td rowspan="3">Yovp</td>
		  <td>Norton AntiVirus</td>
		  <td style="padding: 0px;">
<!--#if expr="${HTTP_USER_AGENT} = /MSIE/" -->
		    <pre style="font-size: smaller;">
<!--#else -->
                    <pre>
<!--#endif -->
<span style="font-size: smaller;">12</span> dosfile.writeline( "command /f /c copy C:\viruz.vbs A:\" )
<span style="font-size: smaller;">13</span> dosfile.writeline( "del C:\viruz.vbs" )</pre>
		  </td>
		</tr>
		<tr>
		  <td>Sophos Antivirus</td>
		  <td><em>The whole body of the malware.</em></td>
		</tr>
		<tr>
		  <td>McAfee Virus Scan</td>
		  <td style="padding: 0px;">
<!--#if expr="${HTTP_USER_AGENT} = /MSIE/" -->
		    <pre style="font-size: smaller;">
<!--#else -->
                    <pre>
<!--#endif -->
<span style="font-size: smaller;"> 1</span> On Error Resume Next
<span style="font-size: smaller;"> 2</span> Dim fso, wsh, dosfile, openvir, copyov
<span style="font-size: smaller;"> 3</span> Set fso = createobject( "scripting.filesystemobject" )
<span style="font-size: smaller;"> 6</span> Set dosfile = fso.createtextfile( "c:\dosfile.bat", true )
<span style="font-size: smaller;"> 7</span> dosfile.writeline( "@echo off" )
<span style="font-size: smaller;"> 8</span> dosfile.writeline( "cd %windir%" )</pre>
		  </td>
		</tr>

	      </tbody>
	    </table>

	    <div class="Caption">
	      <span class="Label">Table 3.</span> Signatures discovered
	      by our signature-extraction algorithm. Where relevant,
	      line numbers for each statement are provided.
	    </div>

	  </div>

	</div>

      </div>

      <!-- ................................................................... -->
      <!-- ................................................................... -->
      <!-- ................................................................... -->

      <div class="Section">
	<a name="sec:future"></a>
	<div class="Title">6. Conclusions and future work</div>

	<p>
	  We present a novel obfuscation-based technique for testing
	  malware detectors. Our testing shows that commercial virus
	  scanners are not resilient to common obfuscations
	  transformations. We also present a signature-extraction
	  algorithm that exploits weaknesses of malware detectors in
	  handling obfuscations.  However, this paper opens several
	  directions for further research. As shown in <a
	    href="#subsec:experiments-signature">Section 5.2</a>, a lot
	  can be learned about a malware detector by using obfuscated
	  variants of a known malware and by guiding the obfuscation to
	  obtain the desired information. A logical next step will be to
	  use more refined techniques once the malware signature is
	  discovered. We will explore techniques from the learning
	  regular languages [<a href="#angluin">1</a>] literature to
	  design better signature-extraction algorithms.  Obfuscations,
	  such as variable renaming, encapsulation of string literals,
	  and reordering, can provide more detailed information about a
	  malware detector. For example, one might inquire whether the
	  signature found is resilient to renaming transformations, or
	  which parts of the signature can withstand encapsulation. The
	  infrastructure for implementing these refinements is in place,
	  so the research focus will be on finding algorithms to guide
	  the refinement of the signature-extraction algorithm.
	</p>

	<p>
	  Another direction for future work is to explore the
	  application of our testing methodology to binary programs,
	  specifically the Intel x86 platform. There is no inherent
	  problem in using the algorithms presented in this paper on x86
	  binaries - all the obfuscation transformations described are
	  applicable to binary programs. We are in the process of
	  developing a binary-rewriting toolkit that will allow us to
	  implement these transformations and to test malware detectors
	  using a larger test suite that includes both x86 binary and
	  Visual Basic malware.
	</p>

      </div>

      <!-- ................................................................... -->
      <!-- ................................................................... -->
      <!-- ................................................................... -->

      <div class="Bibliography">
	<a name="sec:biblio"></a>
	<div class="Title">Bibliography</div>

	<ol>
	  <li class="Item">
	    <a name="angluin"></a>
	    D. Angluin.
	    Learning regular sets from queries and counterexamples.
	    <em>Information and Computation</em>, 75:87-106, 1987.
	  </li>

	  <li class="Item">
	    <a name="brunnstein2002-heureka2"></a>
	    K. Brunnstein.
	    "Heureka-2" AntiVirus Tests.
	    Virus Test Center, University of Hamburg, Computer Science
	    Department, Mar. 2002.
	    Published online at
	    <a href="http://agn-www.informatik.uni-hamburg.de/vtc/en0203.htm">http://agn-www.informatik.uni-hamburg.de/vtc/en0203.htm</a>. Last accessed:
	    16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="chenyu1994-partition"></a>
	    T. Chen and Y. Yu.
	    On the relationship between partition and random testing.
	    <em>IEEE Transactions on Software Engineering</em>, 20(12):977-980,
	    Dec. 1994.<br>
            Online: <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?isNumber=8426&prod=JNL&arnumber=368132&arSt=977&ared=980&arAuthor=Chen%2C+T.Y.%3B+Yu%2C+Y.T.&arNumber=368132&a_id0=368132&a_id1=368133&a_id2=368134&a_id3=368135&a_id4=368136&a_id5=368137&count=6">IEEE Xplore copy</a>
	  </li>

	  <li class="Item">
	    <a name="chowgujohnsonzakharov01-controlobfuscation"></a>
	    S. Chow, Y. Gu, H. Johnson, and V. Zakharov.
	    An approach to the obfuscation of control-flow of sequential computer
	    programs.
	    In G. Davida and Y. Frankel, editors, <em>Proceedings of the 4th
	      International Information Security Conference (ISC'01)</em>, volume 2200 of
	    <em>Lecture Notes in Computer Science</em>, pages 144-155, Malaga, Spain, Oct. 2001.
	    Springer-Verlag.
	  </li>

	  <li class="Item">
	    <a name="collbergthomborsonlow1997-taxonomy"></a>
	    C. Collberg, C. Thomborson, and D. Low.
	    A taxonomy of obfuscating transformations.
	    Technical Report 148, Department of Computer Science, University of
	    Auckland, New Zealand, July 1997.<br>
            Online: <a href="http://www.cs.arizona.edu/~collberg/Research/Publications/CollbergThomborsonLow97a/">C. Collberg's website copy</a>
	  </li>

	  <li class="Item">
	    <a name="collbergthomborsonlow1998-breakingabstractions"></a>
	    C. Collberg, C. Thomborson, and D. Low.
	    Breaking abstractions and unstructuring data structures.
	    In <em>Proceedings of the International Conference on Computer
	      Languages 1998 (ICCL'98)</em>, pages 28-39, Chicago, IL, USA, May 1998. IEEE
	    Computer Society.<br>
            Online: <a href="http://www.cs.arizona.edu/~collberg/Research/Publications/CollbergThomborsonLow97d/">C. Collberg's website copy</a>, <a href="http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=674154&isnumber=14813&punumber=5516&k2dockey=674154@ieeecnfs">IEEE Xplore copy</a>
	  </li>

	  <li class="Item">
	    <a name="collbergthomborsonlow1997-opaque"></a>
	    C. Collberg, C. Thomborson, and D. Low.
	    Manufacturing cheap, resilient, and stealthy opaque constructs.
	    In <em>Proceedings of the 25th Annual ACM SIGPLAN-SIGACT Symposium
	      on Principles of Programming Languages (POPL'98)</em>, San Diego, CA, USA, Jan.
	    1998. ACM Press.<br>
            Online: <a href="http://www.cs.arizona.edu/~collberg/Research/Publications/CollbergThomborsonLow98a/">C. Collberg's website copy</a>, <a href="http://doi.acm.org/10.1145/268946.268962">ACM Digital Library copy</a>
	  </li>

	  <li class="Item">
	    <a name="cooper1976-adaptive"></a>
	    D. W. Cooper.
	    Adaptive testing.
	    In <em>Proceedings of the 2nd International Conference on Software
	      Engineering (ICSE'76)</em>, pages 102-105, San Francisco, CA, USA, Oct. 1976.
	    IEEE Computer Society Press.<br>
            Online: <a href="http://portal.acm.org/citation.cfm?id=800253.807657&dl=portal&dl=ACM&type=series&idx=SERIES402&part=Proceedings&WantType=Proceedings&title=International%20Conference%20on%20Software%20Engineering">ACM Digital Library copy</a>
	  </li>

	  <li class="Item">
	    <a name="detristanulenspiegelmalcomvonunderduk2003-polyshellcode"></a>
	    T. Detristan, T. Ulenspiegel, Y. Malcom, and M. S. von Underduk.
	    Polymorphic shellcode engine using spectrum analysis.
	    <em>Phrack</em>, 11(61), Aug. 2003.
	    Published online at <a href="http://www.phrack.org">http://www.phrack.org</a>. Last accessed: 16
	    Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="duranntafos1984-partition"></a>
	    J. W. Duran and S. C. Ntafos.
	    An evaluation of random testing.
	    <em>IEEE Transactions on Software Engineering</em>, 10(7):438-444, July
	    1984.
	  </li>

	  <li class="Item">
	    <a name="forrestermiller2000-ntfuzz"></a>
	    J. E. Forrester and B. P. Miller.
	    An empirical study of the robustness of Windows NT applications
	    using random testing.
	    In <em>Proceedings of the 4th USENIX Windows Systems Symposium</em>,
	    pages 59-68, Seattle, WA, USA, Aug. 2000.<br>
            Online: <a href="http://www.cs.wisc.edu/~bart/fuzz/fuzz-nt.html">B. P. Miller's website copy</a>, <a href="http://www.usenix.org/events/usenix-win2000/forrester.html">USENIX copy</a>
	  </li>

	  <li class="Item">
	    <a name="franklhamletlittlewoodstrigini1997-reliabilitytesting"></a>
	    P. G. Frankl, R. G. Hamlet, B. Littlewood, and L. Strigini.
	    Choosing a testing method to deliver reliability.
	    In <em>Proceedings of the 19th International Conference on Software
	      Engineering (ICSE'97)</em>, pages 68-78, Boston, MA, USA, May 1997.<br>
            Online: <a href="http://doi.acm.org/10.1145/253228.253244">ACM Digital Library copy</a>
	  </li>

	  <li class="Item">
	    <a name="gordonford1996-avreview"></a>
	    S. Gordon and R. Ford.
	    Real world anti-virus product reviews and evaluations - the current
	    state of affairs.
	    In <em>Proceedings of the 19th National Information Systems Security
	      Conference (NISSC'96)</em>, pages 526-538, Baltimore, MD, USA, Oct. 1996.
	    National Institute of Standards and Technology (NIST).<br>
            Online: <a href="http://csrc.nist.gov/nissc/1996/papers/NISSC96/paper019/final.PDF">NIST PDF copy</a>
	  </li>

	  <li class="Item">
	    <a name="hamlettaylor1990-partition"></a>
	    D. Hamlet and R. Taylor.
	    Partition testing does not inspire confidence.
	    <em>IEEE Transactions on Software Engineering</em>, 16(12):1402-1441,
	    Dec. 1990.<br>
            Online: <a href="http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=62448&k2dockey=62448@ieeejrns">IEEE Xplore copy</a>
	  </li>

	  <li class="Item">
	    <a name="hildebrandtzeller2000-failureinput"></a>
	    R. Hildebrandt and A. Zeller.
	    Simplifying failure-inducing input.
	    In <em>Proceedings of the ACM SIGSOFT International Symposium on
	      Software Testing and Analysis 2000 (ISSTA'00)</em>, pages 135-145, Portland, OR,
	    USA, 2000. ACM Press.<br>
            Online: <a href="http://www.infosun.fmi.uni-passau.de/st/papers/issta2000/">Universit&auml;t Passau copy</a>, <a href="http://doi.acm.org/10.1145/347324.348938">ACM Digital Library copy</a>
	  </li>

	  <li class="Item">
	    <a name="hildebrandtzeller2002-failureinput"></a>
	    R. Hildebrandt and A. Zeller.
	    Simplifying and isolating failure-inducing input.
	    <em>IEEE Transactions on Software Engineering</em>, 28(2):183-200, Feb.
	    2002.<br>
            Online: <a href="http://www.st.cs.uni-sb.de/papers/tse2002/">Universit&auml;t de Saarlandes copy</a>, <a href="http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=988498&isnumber=21282&punumber=32&k2dockey=988498@ieeejrns">IEEE Xplore copy</a>
	  </li>

	  <li class="Item">
	    <a name="icsa-certification"></a>
	    ICSA Labs.
	    Anti-virus product certification.
	    Published online at
	    <a href="http://www.icsalabs.com/html/communities/antivirus/certification.shtml">http://www.icsalabs.com/html/communities/antivirus/certification.shtml</a>.
	    Last accessed: 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="kaspersky2002-epo"></a>
	    E. Kaspersky.
	    <em>Virus List Encyclopedia</em>, chapter Ways of Infection: Viruses
	    without an Entry Point.
	    Kaspersky Labs, 2002.<br>
            Online: <a href="http://www.viruslist.com/eng/viruslistbooks.html?id=32">VirusList.com</a>
	  </li>

	  <li class="Item">
	    <a name="lurhq2003-sobig.a"></a>
	    LURHQ Threat Intelligence Group.
	    Sobig.a and the spam you received today.
	    Technical report, LURHQ, 2003.
	    Published online at <a href="http://www.lurhq.com/sobig.html">http://www.lurhq.com/sobig.html</a>. Last
	    accessed on 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="lurhq2003-sobig.e"></a>
	    LURHQ Threat Intelligence Group.
	    Sobig.e - Evolution of the worm.
	    Technical report, LURHQ, 2003.
	    Published online at <a href="http://www.lurhq.com/sobig-e.html">http://www.lurhq.com/sobig-e.html</a>. Last
	    accessed on 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="lurhq2003-sobig.f"></a>
	    LURHQ Threat Intelligence Group.
	    Sobig.f examined.
	    Technical report, LURHQ, 2003.
	    Published online at <a href="http://www.lurhq.com/sobig-f.html">http://www.lurhq.com/sobig-f.html</a>. Last
	    accessed on 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="marinescu2003-stepar"></a>
	    A. Marinescu.
	    Russian doll.
	    <em>Virus Bulletin</em>, pages 7-9, Aug. 2003.<br>
            Online: <a href="http://www.virusbtn.com/resources/viruses/indepth/stepar.xml">Virus Bulletin Archive copy</a>
	  </li>

	  <li class="Item">
	    <a name="marx2000-avtesting"></a>
	    A. Marx.
	    A guideline to anti-malware-software testing.
	    In <em>Proceedings of the 9th Annual European Institute for
	      Computer Antivirus Research Conference (EICAR'00)</em>, 2000.<br>
            Online: <a href="http://www.av-test.org/sites/references_papers.php3?lang=en">AV-Test.org copy</a>
	  </li>

	  <li class="Item">
	    <a name="marx2002-avretrotest"></a>
	    A. Marx.
	    Retrospective testing - how good heuristics really work.
	    In <em>Proceedings of the 2002 Virus Bulletin Conference (VB2002)</em>,
	    New Orleans, LA, USA, Sept. 2002. Virus Bulletin.<br>
            Online: <a href="http://www.av-test.org/sites/references_papers.php3?lang=en">AV-Test.org copy</a>
	  </li>

	  <li class="Item">
	    <a name="mcafee-avert"></a>
	    McAfee AVERT.
	    Virus information library.
	    Published online at <a href="http://us.mcafee.com/virusInfo/default.asp">http://us.mcafee.com/virusInfo/default.asp</a>.
	    Last accessed: 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="mcgrawmorrisett2000-taxonomy"></a>
	    G. McGraw and G. Morrisett.
	    Attacking malicious code: report to the Infosec research council.
	    <em>IEEE Software</em>, 17(5):33 - 41, Sept./Oct. 2000.<br>
	    Online: <a href="http://www.infosec-research.org/docs_public/ISTSG-MC-report.pdf">InfoSec Research Council PDF copy</a>
	  </li>

	  <li class="Item">
	    <a name="millerfredriksenso1990-unixfuzz"></a>
	    B. P. Miller, L. Fredriksen, and B. So.
	    An empirical study of the reliability of UNIX utilities.
	    <em>Communications of the ACM</em>, 33(12):12-44, Dec. 1990.<br>
            Online: <a href="http://doi.acm.org/10.1145/96267.96279">ACM Digital Library copy</a>
	  </li>

	  <li class="Item">
	    <a name="millerkoskileemagantymurthynatarajansteidl1995-x11fuzz"></a>
	    B. P. Miller, D. Koski, C. P. Lee, V. Maganty, R. Murthy, A. Natarajan, and
	    J. Steidl.
	    Fuzz revisited: A re-examination of the reliability of UNIX
	    utilities and services.
	    Technical Report 1268, University of Wisconsin, Madison, Computer
	    Sciences Department, Madison, WI, USA, Apr. 1995.<br>
            Online: <a href="http://www.cs.wisc.edu/~bart/fuzz/">B. P. Miller's website copy</a>
	  </li>

	  <li class="Item">
	    <a name="myers1979-testing"></a>
	    G. J. Myers.
	    <em>The Art of Software Testing</em>.
	    John Wiley & Sons, first edition, Feb. 1979.<br>
            Online: <a href="http://www.amazon.com/exec/obidos/ASIN/0471043281/mihaiswebsite-20">Amazon.com page</a>, <a href="http://service.bfast.com/bfast/click?bfmid=2181&sourceid=41183548&bfpid=0471043281&bfmtype=book">Barnes & Noble.com page</a>
<P>
	  </li>

	  <li class="Item">
	    <a name="ntafos1998-randompartition"></a>
	    S. C. Ntafos.
	    On random and partition testing.
	    In <em>Proceedings of the ACM SIGSOFT International Symposium on
	      Software Testing and Analysis 1998 (ISSTA'98)</em>, pages 42-48, Clearwater
	    Beach, FL, USA, Mar. 1998. ACM Press.<br>
            Online: <a href="http://doi.acm.org/10.1145/271771.271785">ACM Digital Library copy</a>
	  </li>

	  <li class="Item">
	    <a name="ntafos2001-partition"></a>
	    S. C. Ntafos.
	    On comparisons of random, partition, and proportional partition
	    testing.
	    <em>IEEE Transactions on Software Engineering</em>, 27(10):949-960,
	    Oct. 2001.<br>
            Online: <a href="http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=962563&isnumber=20775&punumber=32&k2dockey=962563@ieeejrns&arSt=949&ared=960&arAuthor=Ntafos%2C+S.C.%3B">IEEE Xplore copy</a>
	  </li>

	  <li class="Item">
	    <a name="sarc-virencyclopaedia"></a>
	    Symantec Antivirus Research Center.
	    Expanded threat list and virus encyclopedia.
	    Published online at
	    <a href="http://securityresponse.symantec.com/avcenter/venc/data/cih.html">http://securityresponse.symantec.com/avcenter/venc/data/cih.html</a>. Last
	    accessed: 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="szorferrie2001-metamorphic"></a>
	    P. Sz&ouml;r and P. Ferrie.
	    Hunting for metamorphic.
	    In <em>Proceedings of 2001 Virus Bulletin Conference (VB2001)</em>,
	    pages 123 - 144, September 2001.<br>
            Online: <a href="http://www.peterszor.com/metamorp.pdf">P. Sz&ouml;r's website PDF copy</a>
	  </li>

	  <li class="Item">
	    <a name="teso-burneye"></a>
	    TESO.
	    Burneye ELF encryption program.
	    Published online at <a href="http://teso.scene.at">http://teso.scene.at</a>. Last accessed: 15
	    Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="wildlist"></a>
	    The WildList Organization International.
	    Frequently asked questions.
	    Published online at <a href="http://www.wildlist.org/faq.htm">http://www.wildlist.org/faq.htm</a>. Last
	    accessed: 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="vb-certification"></a>
	    Virus Bulletin.
	    VB 100% Award.
	    Published online at
	    <a href="http://www.virusbtn.com/vb100/about/100use.xml">http://www.virusbtn.com/vb100/about/100use.xml</a>. Last accessed: 16 Jan.
	    2004.
	  </li>

	  <li class="Item">
	    <a name="wang2000-security4survivability"></a>
	    C. Wang.
	    <em>A security architecture for survivability mechanisms</em>.
	    PhD thesis, University of Virginia, Oct. 2000.<br>
            Online: <a href="http://dependability.cs.virginia.edu/publications/wangthesis.pdf">University of Virginia PDF copy</a>
	  </li>

	  <li class="Item">
	    <a name="checkmark-certification-av1"></a>
	    West Coast Labs.
	    Anti-virus Checkmark level 2.
	    Published online at
	    <a href="http://www.check-mark.com/checkmark/pdf/Checkmark_AV1.pdf">http://www.check-mark.com/checkmark/pdf/Checkmark_AV1.pdf</a>. Last
	    accessed: 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="checkmark-certification-av2"></a>
	    West Coast Labs.
	    Anti-virus Checkmark level 2.
	    Published online at
	    <a href="http://www.check-mark.com/checkmark/pdf/Checkmark_AV2.pdf">http://www.check-mark.com/checkmark/pdf/Checkmark_AV2.pdf</a>. Last
	    accessed: 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="weyukerjeng1991-partition"></a>
	    E. J. Weyuker and B. Jeng.
	    Analyzing partition testing strategies.
	    <em>IEEE Transactions on Software Engineering</em>, 17(7):703-711, July
	    1991.<br>
            Online: <a href="http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=83906&isnumber=2738&punumber=32&k2dockey=83906@ieeejrns&arSt=703&ared=711&arAuthor=Weyuker%2C+E.J.%3B+Jeng%2C+B.%3B">IEEE Xplore copy</a>
	  </li>

	  <li class="Item">
	    <a name="wroblewski2002-generalobfuscation"></a>
	    G. Wroblewski.
	    <em>General method of program code obfuscation</em>.
	    PhD thesis, Institute of Engineering Cybernetics, Wroclaw University
	    of Technology, Wroclaw, Poland, 2002.<br>
            Online: <a href="http://www.mysz.org/publications.html">G. Wroblewski's website copy</a>
	  </li>

	  <li class="Item">
	    <a name="z0mbie-mistfall"></a>
	    z0mbie.
	    Automated reverse engineering: Mistfall engine.
	    Published online at <a href="http://z0mbie.host.sk/autorev.txt">http://z0mbie.host.sk/autorev.txt</a>. Last
	    accessed: 16 Jan. 2004.
	  </li>

	  <li class="Item">
	    <a name="z0mbie-homepage"></a>
	    z0mbie.
	    z0mbie's homepage.
	    Published online at <a href="http://z0mbie.host.sk">http://z0mbie.host.sk</a>. Last accessed: 16
	    Jan. 2004.
	  </li>
	</ol>

      </div>

      <!-- ................................................................... -->
      <!-- ................................................................... -->
      <!-- ................................................................... -->

      <div class="Notes">
	<a name="sec:notes"></a>
	<div class="Title">Notes</div>

	<ol>

	  <li>
	    <a name="footnote:1"></a> ``Hacker'' and ``blackhat'' will
	    be used synonymously throughout this paper.
	  </li>

	  <li>
	    <a name="footnote:2"></a> During the editing of this paper,
	    after filling out <a href="#t:avsigs">Table 3</a> and saving
	    the document to disk, the anti-virus tool installed on one
	    of the authors' machine identified this section of the HTML
	    document as infected and quarantined it!
	  </li>

	</ol>

      </div>

    </div>
    </center>

    <!-- ................................................................... -->
    <!-- ................................................................... -->
    <!-- ................................................................... -->

    <div class="MaintainerInfo">
      <div>
	<div>
	  <div>
	    Copyright 2004 Mihai Christodorescu. All rights reserved. <br>
	    Maintained by Mihai Christodorescu (<a href="http://www.cs.wisc.edu/~mihai">http://www.cs.wisc.edu/~mihai</a>). <br>
            Created: May 07 23:13:17 CDT 2004<br>
<!-- Created: Fri May 07 23:13:17 Central Daylight Time 2004 -->
<!-- hhmts start -->
Last modified: Fri Nov 18 00:00:50 CST 2005
<!-- hhmts end -->
	  </div>
	</div>
      </div>

  </body>
</html>
